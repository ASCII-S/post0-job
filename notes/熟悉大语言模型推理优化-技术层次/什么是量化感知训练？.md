---
created: '2025-10-19'
last_reviewed: null
next_review: '2025-10-19'
review_count: 0
difficulty: medium
mastery_level: 0.0
tags:
- 熟悉大语言模型推理优化-技术层次
- 熟悉大语言模型推理优化-技术层次/什么是量化感知训练？.md
related_outlines: []
---
# 什么是量化感知训练？

## 面试标准答案

量化感知训练（Quantization-Aware Training, QAT）是一种在模型训练过程中模拟量化操作的技术。它通过在前向传播中插入"伪量化"（Fake Quantization）节点来模拟低比特推理时的精度损失，同时在反向传播中使用直通估计器（Straight-Through Estimator, STE）传递梯度，让模型权重在训练中自适应地调整以补偿量化误差。这使得量化后的模型能够在INT8甚至INT4精度下保持接近FP32的准确率。

## 详细讲解

### 1. QAT的核心机制

#### 1.1 伪量化（Fake Quantization）

伪量化是QAT的核心，它在训练时模拟推理时的量化过程，但仍保持浮点数格式进行计算。

**数学表达**：
```
量化：   x_int = round((x_fp32 - zero_point) / scale)
反量化： x_fake = x_int * scale + zero_point
```

这个"量化-反量化"过程引入了舍入误差，让模型在训练时就"看到"量化带来的噪声。

**具体例子**：

假设我们有一个权重矩阵 `W`，形状为 `[4096, 4096]`（Transformer FFN层典型大小）：

```python
# 原始FP32权重
W_fp32 = [[-0.523, 0.891, -0.234, ...],  # shape: [4096, 4096]
          [0.456, -0.123, 0.789, ...],
          ...]

# INT8量化参数（对称量化）
scale = 0.0042  # 从权重范围计算得到
zero_point = 0  # 对称量化

# 伪量化过程
W_int = round(W_fp32 / scale)  # [-124, 212, -56, ...] (INT8范围 -128~127)
W_fake = W_int * scale         # [-0.5208, 0.8904, -0.2352, ...] (略有误差)

# 训练中使用 W_fake 进行前向传播
```

可以看到，`-0.523` 被量化为 `-124`，反量化后变成 `-0.5208`，产生了 `0.0022` 的误差。

#### 1.2 直通估计器（STE）

量化是一个不可微的离散操作（round函数），无法直接反向传播梯度。STE的解决方案是：**在前向传播时执行量化，在反向传播时假装量化不存在**。

**梯度传递示意**：
```
前向： x → quantize(x) → y
反向： ∂L/∂x ← ∂L/∂y (直接传递，忽略量化)
```

**具体例子**：

```python
class FakeQuantize(torch.autograd.Function):
    @staticmethod
    def forward(ctx, x, scale):
        # 前向：量化再反量化
        x_int = torch.round(x / scale)
        x_fake = x_int * scale
        return x_fake
    
    @staticmethod
    def backward(ctx, grad_output):
        # 反向：梯度直接传递（STE）
        return grad_output, None  # None对应scale参数

# 使用示例
x = torch.tensor([0.523, -0.891], requires_grad=True)
scale = 0.01
y = FakeQuantize.apply(x, scale)  # 前向：[0.52, -0.89]
loss = y.sum()
loss.backward()
print(x.grad)  # [1., 1.] 梯度未被量化影响
```

### 2. QAT在Transformer各层的具体应用

#### 2.1 线性层（Linear Layer）

这是最主要的量化目标，占模型计算的90%以上。

**Layer结构**：
```python
# BERT-base的一个FFN层
fc1 = nn.Linear(768, 3072)  # 权重形状: [3072, 768]
fc2 = nn.Linear(3072, 768)  # 权重形状: [768, 3072]
```

**QAT改造**：
```python
class QATLinear(nn.Module):
    def __init__(self, in_features, out_features):
        super().__init__()
        self.weight = nn.Parameter(torch.randn(out_features, in_features))
        self.bias = nn.Parameter(torch.zeros(out_features))
        
        # 量化参数（可学习或固定）
        self.weight_scale = nn.Parameter(torch.tensor(0.01))
        self.activation_scale = nn.Parameter(torch.tensor(0.05))
    
    def forward(self, x):
        # 输入x形状: [batch_size, seq_len, in_features]
        # 例如: [32, 128, 768]
        
        # 1. 量化权重
        w_int = torch.round(self.weight / self.weight_scale)
        w_int = torch.clamp(w_int, -128, 127)  # INT8范围
        w_fake = w_int * self.weight_scale  # [3072, 768]
        
        # 2. 正常矩阵乘法
        out = F.linear(x, w_fake, self.bias)  # [32, 128, 3072]
        
        # 3. 量化激活值
        out_int = torch.round(out / self.activation_scale)
        out_int = torch.clamp(out_int, -128, 127)
        out_fake = out_int * self.activation_scale  # [32, 128, 3072]
        
        return out_fake
```

**具体数值例子**：
```python
# 输入激活值（batch=2, seq_len=4, hidden=768）
x = torch.randn(2, 4, 768) * 2  # 范围约[-6, 6]

# 权重（3072, 768）
W = torch.randn(3072, 768) * 0.02  # 范围约[-0.06, 0.06]

# 量化参数
weight_scale = W.abs().max() / 127  # ≈ 0.00047
activation_scale = x.abs().max() / 127  # ≈ 0.047

# 量化权重
W_int = (W / weight_scale).round().clamp(-128, 127)  # INT8
W_fake = W_int * weight_scale

# 前向计算
output = x @ W_fake.T  # [2, 4, 3072]

# 量化输出激活
output_int = (output / activation_scale).round().clamp(-128, 127)
output_fake = output_int * activation_scale
```

#### 2.2 Attention层

Attention机制中有多个需要量化的矩阵运算。

**Attention结构**（以BERT为例）：
```python
class QATAttention(nn.Module):
    def __init__(self, hidden_size=768, num_heads=12):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = hidden_size // num_heads  # 64
        
        # QKV投影矩阵
        self.q_proj = QATLinear(768, 768)  # [768, 768]
        self.k_proj = QATLinear(768, 768)
        self.v_proj = QATLinear(768, 768)
        self.o_proj = QATLinear(768, 768)  # 输出投影
        
        # Attention矩阵的量化参数
        self.attn_scale = nn.Parameter(torch.tensor(0.1))
    
    def forward(self, x):
        # x: [batch, seq_len, hidden] = [32, 128, 768]
        batch, seq_len, hidden = x.shape
        
        # 1. 计算QKV（每个都被量化）
        Q = self.q_proj(x)  # [32, 128, 768] 已量化
        K = self.k_proj(x)  # [32, 128, 768] 已量化
        V = self.v_proj(x)  # [32, 128, 768] 已量化
        
        # 2. Reshape为多头
        Q = Q.view(batch, seq_len, self.num_heads, self.head_dim)
        Q = Q.transpose(1, 2)  # [32, 12, 128, 64]
        K = K.view(batch, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        V = V.view(batch, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        
        # 3. Attention分数（这里也需要量化！）
        attn = torch.matmul(Q, K.transpose(-2, -1))  # [32, 12, 128, 128]
        attn = attn / (self.head_dim ** 0.5)
        
        # 量化attention分数（关键！）
        attn_int = torch.round(attn / self.attn_scale)
        attn_int = torch.clamp(attn_int, -128, 127)
        attn_fake = attn_int * self.attn_scale
        
        # 4. Softmax（通常保持FP32）
        attn_weights = F.softmax(attn_fake, dim=-1)  # [32, 12, 128, 128]
        
        # 5. 加权求和
        out = torch.matmul(attn_weights, V)  # [32, 12, 128, 64]
        out = out.transpose(1, 2).contiguous().view(batch, seq_len, hidden)
        
        # 6. 输出投影（量化）
        out = self.o_proj(out)  # [32, 128, 768]
        
        return out
```

**关键量化点**：
1. **QKV投影矩阵**：3个 `[768, 768]` 的权重矩阵
2. **Attention分数矩阵**：`[batch, heads, seq, seq]` = `[32, 12, 128, 128]`，这个矩阵很大，量化收益明显
3. **输出投影**：1个 `[768, 768]` 的权重矩阵

#### 2.3 LayerNorm层

LayerNorm通常保持FP32精度，但在激进量化中也会被量化。

```python
class QATLayerNorm(nn.Module):
    def __init__(self, hidden_size=768):
        super().__init__()
        self.weight = nn.Parameter(torch.ones(hidden_size))
        self.bias = nn.Parameter(torch.zeros(hidden_size))
        self.eps = 1e-12
        
        # LayerNorm输出的量化参数
        self.output_scale = nn.Parameter(torch.tensor(0.02))
    
    def forward(self, x):
        # x: [32, 128, 768]
        
        # LayerNorm计算（保持FP32）
        mean = x.mean(-1, keepdim=True)  # [32, 128, 1]
        var = x.var(-1, keepdim=True)    # [32, 128, 1]
        x_norm = (x - mean) / torch.sqrt(var + self.eps)  # [32, 128, 768]
        out = self.weight * x_norm + self.bias
        
        # 量化输出
        out_int = torch.round(out / self.output_scale)
        out_int = torch.clamp(out_int, -128, 127)
        out_fake = out_int * self.output_scale
        
        return out_fake
```

### 3. 完整Transformer Block的QAT示例

```python
class QATTransformerBlock(nn.Module):
    def __init__(self, hidden_size=768, num_heads=12, intermediate_size=3072):
        super().__init__()
        
        # Self-Attention
        self.attention = QATAttention(hidden_size, num_heads)
        self.attn_ln = QATLayerNorm(hidden_size)
        
        # Feed-Forward Network
        self.ffn_fc1 = QATLinear(hidden_size, intermediate_size)      # [3072, 768]
        self.ffn_fc2 = QATLinear(intermediate_size, hidden_size)      # [768, 3072]
        self.ffn_ln = QATLayerNorm(hidden_size)
        
        # 激活函数（GELU的量化）
        self.gelu_scale = nn.Parameter(torch.tensor(0.05))
    
    def forward(self, x):
        # x: [batch, seq_len, hidden] = [32, 128, 768]
        
        # 1. Self-Attention + Residual
        attn_out = self.attention(x)           # [32, 128, 768] 量化
        x = x + attn_out                       # 残差连接
        x = self.attn_ln(x)                    # [32, 128, 768] 量化
        
        # 2. Feed-Forward Network
        ffn_hidden = self.ffn_fc1(x)           # [32, 128, 3072] 量化
        
        # GELU激活（量化）
        ffn_hidden = F.gelu(ffn_hidden)        # [32, 128, 3072]
        gelu_int = torch.round(ffn_hidden / self.gelu_scale)
        gelu_int = torch.clamp(gelu_int, -128, 127)
        ffn_hidden = gelu_int * self.gelu_scale
        
        ffn_out = self.ffn_fc2(ffn_hidden)     # [32, 128, 768] 量化
        
        # 3. Residual + LayerNorm
        x = x + ffn_out
        x = self.ffn_ln(x)                     # [32, 128, 768] 量化
        
        return x
```

**该Block中的量化点统计**：
- **权重矩阵**：7个（Q、K、V、O、FC1、FC2 + LayerNorm的weight）
- **激活值**：每次Linear、LayerNorm、GELU输出都量化
- **总量化操作**：约12-15次伪量化/每个token

### 4. 训练流程示例

```python
import torch
import torch.nn as nn
import torch.optim as optim

# 1. 创建QAT模型
model = QATTransformerBlock(hidden_size=768, num_heads=12)

# 2. 优化器（注意学习率通常比普通训练小）
optimizer = optim.Adam(model.parameters(), lr=1e-5)  # 比FP32训练小10倍

# 3. 训练循环
for epoch in range(num_epochs):
    for batch_idx, (inputs, labels) in enumerate(dataloader):
        # inputs: [32, 128, 768]
        # labels: [32, 128]
        
        # 前向传播（包含伪量化）
        outputs = model(inputs)  # [32, 128, 768]
        
        # 计算loss
        loss = criterion(outputs, labels)
        
        # 反向传播（梯度通过STE传递）
        optimizer.zero_grad()
        loss.backward()
        
        # 梯度裁剪（QAT训练中很重要！）
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        
        # 更新参数
        optimizer.step()
        
        # 量化参数的范围约束
        with torch.no_grad():
            for name, param in model.named_parameters():
                if 'scale' in name:
                    param.clamp_(min=1e-6)  # 防止scale过小
```

### 5. QAT的量化参数学习

**两种策略**：

#### 5.1 固定量化参数
```python
# 在训练前用校准数据确定scale
def calibrate_scale(model, calibration_data):
    with torch.no_grad():
        for batch in calibration_data:
            _ = model(batch)
            # 收集激活值范围
    
    # 为每层设置scale
    for layer in model.modules():
        if isinstance(layer, QATLinear):
            max_val = layer.weight.abs().max()
            layer.weight_scale.data = max_val / 127
            layer.weight_scale.requires_grad = False  # 固定
```

#### 5.2 可学习量化参数
```python
# scale作为可学习参数
class LearnableQATLinear(nn.Module):
    def __init__(self, in_features, out_features):
        super().__init__()
        self.weight = nn.Parameter(torch.randn(out_features, in_features))
        
        # 初始化为合理值
        initial_scale = self.weight.abs().max() / 127
        self.weight_scale = nn.Parameter(torch.tensor(initial_scale))
        
        # 可学习！
        self.weight_scale.requires_grad = True
    
    def forward(self, x):
        # scale会在训练中自动调整
        w_int = torch.round(self.weight / self.weight_scale)
        w_fake = w_int * self.weight_scale
        return F.linear(x, w_fake)
```

### 6. 不同层的量化敏感度

在实际应用中，不同层对量化的敏感度差异很大：

| 层类型            | 典型形状                                            | 量化难度 | 推荐策略           |
| ----------------- | --------------------------------------------------- | -------- | ------------------ |
| **Embedding**     | `[vocab_size, hidden]`<br/>`[30522, 768]`           | 低       | INT8甚至INT4       |
| **Q/K/V投影**     | `[hidden, hidden]`<br/>`[768, 768]`                 | 中等     | INT8，K/V可更低    |
| **Attention矩阵** | `[batch, heads, seq, seq]`<br/>`[32, 12, 128, 128]` | 高       | 保持FP16或动态量化 |
| **FFN FC1**       | `[intermediate, hidden]`<br/>`[3072, 768]`          | 低       | INT8               |
| **FFN FC2**       | `[hidden, intermediate]`<br/>`[768, 3072]`          | 低       | INT8               |
| **LayerNorm**     | `[hidden]`<br/>`[768]`                              | 高       | 保持FP32           |
| **输出层**        | `[vocab_size, hidden]`<br/>`[30522, 768]`           | 高       | INT8或混合精度     |

**混合精度QAT示例**：
```python
class MixedPrecisionTransformer(nn.Module):
    def __init__(self):
        super().__init__()
        
        # 低敏感度层：INT8
        self.q_proj = QATLinear(768, 768, bits=8)
        self.ffn = QATLinear(768, 3072, bits=8)
        
        # 高敏感度层：FP16
        self.layernorm = nn.LayerNorm(768, dtype=torch.float16)
        
        # 中等敏感度：动态INT8
        self.attn_matmul = DynamicQATMatMul(bits=8)
```

### 7. QAT vs PTQ 具体对比

**在BERT-base（110M参数）上INT8量化的实际表现**：

| 场景       | PTQ精度 | QAT精度 | 训练时间        |
| ---------- | ------- | ------- | --------------- |
| GLUE平均分 | -1.2%   | -0.2%   | 0 vs 8 GPU小时  |
| SQuAD F1   | -2.1%   | -0.5%   | 0 vs 12 GPU小时 |
| INT4量化   | -8.5%   | -2.3%   | 0 vs 24 GPU小时 |

### 8. 实际部署：从QAT模型到INT8推理

训练完成后，需要将QAT模型转换为真正的INT8模型：

```python
def export_to_int8(qat_model):
    int8_model = {}
    
    for name, module in qat_model.named_modules():
        if isinstance(module, QATLinear):
            # 提取量化权重
            scale = module.weight_scale.item()
            weight_int8 = torch.round(module.weight / scale).clamp(-128, 127)
            
            int8_model[name] = {
                'weight': weight_int8.to(torch.int8),  # 真正的INT8
                'scale': scale,
                'bias': module.bias
            }
    
    return int8_model

# 推理时使用真正的INT8算子
def int8_linear(x_int8, weight_int8, x_scale, w_scale, bias):
    # 使用INT8矩阵乘法（cuBLAS的cublasGemmEx）
    out_int32 = torch.ops.aten.matmul_int8(x_int8, weight_int8)  # INT32累加
    
    # 反量化到FP32
    out_fp32 = out_int32.float() * (x_scale * w_scale) + bias
    
    return out_fp32
```

### 9. 总结

量化感知训练的核心是让模型在**训练时就看到量化误差**，通过数百万次迭代学习如何适应这些误差。具体来说：

1. **在Linear层**（形状`[out, in]`）的权重矩阵上进行INT8量化
2. **在Attention**的QKV投影（各`[768, 768]`）和注意力分数矩阵（`[batch, heads, seq, seq]`）上量化
3. **在FFN**的两个线性层（`[3072, 768]`和`[768, 3072]`）上量化
4. **使用STE**让梯度穿过不可微的round操作
5. **通过数千步训练**让权重分布自适应量化约束

这使得在推理时可以用真正的INT8算子（如Tensor Core的INT8 GEMM），获得2-4倍加速和4倍显存节省，同时精度损失控制在1%以内。


---

## 相关笔记
<!-- 自动生成 -->

- [QAT中的fake_quantization是如何工作的？](notes/熟悉大语言模型推理优化-技术层次/QAT中的fake_quantization是如何工作的？.md) - 相似度: 33% | 标签: 熟悉大语言模型推理优化-技术层次, 熟悉大语言模型推理优化-技术层次/QAT中的fake_quantization是如何工作的？.md

