---
created: '2025-10-19'
last_reviewed: null
next_review: '2025-10-19'
review_count: 0
difficulty: medium
mastery_level: 0.0
tags:
- 熟悉大语言模型推理优化-技术层次
- 熟悉大语言模型推理优化-技术层次/序列并行是什么？.md
related_outlines: []
---
# 序列并行是什么？

## 面试标准答案

序列并行（Sequence Parallelism）是一种将输入序列在序列长度维度上切分到多个设备的并行策略。不同于张量并行在特征/参数维度切分、流水线并行在层维度切分，序列并行将[B,S,H]张量沿S维度分割为[B,S/N,H]，每个GPU处理不同的序列位置。主要用于优化非矩阵乘法算子（LayerNorm、Dropout）的激活显存，特别适合超长序列场景。通常与张量并行结合使用，通过All-to-All通信在序列分片和特征分片间转换，可将激活显存降低至原来的1/N。

---

## 详细讲解

### 1. 核心概念

序列并行是针对Transformer模型中序列维度的并行化策略，通过将输入序列在序列长度上进行切分，将不同的token分配到不同的设备上并行处理。

#### 基本切分方式

```python
# 原始张量形状
Input: [Batch_size, Seq_len, Hidden_dim]
# 例如: [32, 2048, 12288]

# 序列并行 (4-way)
# 每个GPU处理 1/4 的序列长度
GPU 0: [32,    0: 512, 12288]
GPU 1: [32,  512:1024, 12288]
GPU 2: [32, 1024:1536, 12288]
GPU 3: [32, 1536:2048, 12288]
```

### 2. 为什么需要序列并行？

#### 激活显存瓶颈

在大模型训练/推理中，激活值（activation）占用大量显存：

```
示例：GPT-3 175B 模型
- 序列长度：2048
- 批大小：32
- 隐藏维度：12288
- 层数：96

无并行时每层激活显存：
32 × 2048 × 12288 × 2 bytes = 1.6 GB

仅张量并行(8-way)：
- 线性层输出: [32, 2048, 1536] = 0.2 GB ✓
- 但LayerNorm输入需All-Reduce回: [32, 2048, 12288] = 1.6 GB ✗
- 瓶颈: LayerNorm等算子仍需完整Hidden维度

张量并行 + 序列并行(8-way)：
- LayerNorm输入: [32, 256, 12288] = 0.2 GB ✓
- 在序列维度切分，避免All-Reduce
- 总节省: 从1.6GB降至0.2GB (87.5%)
```

#### 与其他并行方式的区别

| 并行策略       | 切分维度     | 主要优化目标          | 典型应用          |
| -------------- | ------------ | --------------------- | ----------------- |
| **数据并行**   | Batch维度    | 训练吞吐量            | 小模型训练        |
| **张量并行**   | Hidden维度   | 参数显存 + 线性层激活 | 矩阵乘法          |
| **流水线并行** | Layer维度    | 参数显存              | 深度模型          |
| **序列并行**   | Sequence维度 | LayerNorm等算子的激活 | 长序列/非矩阵算子 |

**关键问题**：为什么张量并行不够？

```python
# Transformer层的计算流程

# 1. 线性层 (张量并行有效)
X: [B, S, H]
W: [H, H/N]  # 按列切分
Y = X @ W → [B, S, H/N]  # ✓ 激活已减少

# 2. All-Reduce (张量并行的必要步骤)
Y_full = All_Reduce(Y) → [B, S, H]  # ✗ 又变大了！

# 3. LayerNorm (需要完整Hidden维度)
# LayerNorm在最后一个维度归一化，需要完整的H
mean = Y_full.mean(dim=-1)  # 需要完整的H维度
std = Y_full.std(dim=-1)    # 需要完整的H维度

# 序列并行的解决方案：
# 不在Hidden维度切分，而是在Sequence维度切分
Y_sp: [B, S/N, H]  # 每个GPU处理不同的token
# LayerNorm仍然在H维度计算，不需要通信！
```

### 3. 适用场景与算子

#### 直接适用的算子

序列并行对**逐元素操作**和**序列局部操作**最有效：

```python
# ✅ LayerNorm (在hidden维度归一化)
# 输入: [B, S/N, H] 
# 每个GPU独立计算其序列片段
mean = x.mean(dim=-1)  # 只需hidden维度
std = x.std(dim=-1)

# ✅ Dropout (逐元素随机失活)
mask = torch.rand_like(x) > dropout_rate

# ✅ 激活函数 (逐元素)
output = F.gelu(x)

# ✅ 残差连接
output = x + residual
```

#### 需要特殊处理的算子

```python
# ❌ Self-Attention (需要全序列交互)
# Q, K, V: [B, S/N, H] 
# 计算 QK^T 需要完整的 K: [B, S, H]
# 解决: All-Gather 收集完整序列

# ❌ 矩阵乘法 (与张量并行冲突)
# X: [B, S/N, H_in]
# W: [H_in, H_out]
# 如果W按列切分(张量并行)，需要转换布局
```

### 4. 与张量并行的结合

序列并行通常不单独使用，而是与张量并行结合以优化整体显存：

#### 布局转换流程

```python
# Transformer Layer 流程

# 1. 输入状态: 序列分片
X: [B, S/N, H]  # 每个GPU有不同序列片段

# 2. 准备进入矩阵乘法 (需要张量并行)
X_gather = All_Gather(X, dim=1)  # [B, S, H]
X_tp = Split(X_gather, dim=-1)   # [B, S, H/N] 特征分片

# 3. 矩阵乘法 (张量并行)
Y = MatMul(X_tp, W_col)  # W按列切分

# 4. 准备进入LayerNorm (需要序列并行)
Y_gather = All_Gather(Y, dim=-1)  # [B, S, H]
Y_sp = Split(Y_gather, dim=1)     # [B, S/N, H] 序列分片

# 5. LayerNorm (序列并行)
Y_norm = LayerNorm(Y_sp)
```

#### Megatron-LM的高效实现

```python
# 通过融合通信避免多次All-Gather

# TP的输出 Reduce-Scatter: [B, S, H] → [B, S/N, H]
# 同时完成:
# 1. 跨TP组的All-Reduce (累加结果)
# 2. 序列维度的Scatter (分配序列片段)

output = Reduce_Scatter_along_seq(
    tensor=tp_output,      # [B, S, H/N]
    tp_group=tp_group,     # 张量并行通信组
    seq_parallel=True
)
# 直接得到: [B, S/N, H]
```

### 5. 显存与通信分析

#### 显存节省

```python
# 以 GPT-3 175B 为例
batch_size = 32
seq_len = 2048
hidden = 12288
num_layers = 96
world_size = 8

# 每层激活显存

# 仅张量并行
act_per_layer = batch_size * seq_len * hidden * 2  # 1.6 GB
total_act = act_per_layer * num_layers  # 153.6 GB per GPU

# 张量并行 + 序列并行
act_per_layer = batch_size * (seq_len//world_size) * hidden * 2  # 0.2 GB
total_act = act_per_layer * num_layers  # 19.2 GB per GPU

# 节省: 87.5% 激活显存
```

#### 通信开销

```python
# All-to-All 通信量 (特征分片 ↔ 序列分片)
comm_size = batch_size * seq_len * hidden * sizeof(dtype)

# 示例: [32, 2048, 12288], bfloat16
comm_size = 32 * 2048 * 12288 * 2 = 1.6 GB

# 频率: 每层2次 (进入TP前 + 退出TP后)
total_comm = 1.6 GB * 2 * 96 layers = 307.2 GB

# 对于NVLink(600GB/s), 通信时间: ~0.5s
```

### 6. 推理场景的特殊性

#### 推理时的限制

```python
# 训练 vs 推理

# 训练:
# - 序列长度固定 (如2048)
# - 批大小较大 (32-64)
# - 激活显存是主要瓶颈
# ✅ 序列并行收益大

# 推理 (Prefill阶段):
# - 输入序列长度变化 (几百到几千)
# - 批大小通常较小 (1-8)
# ⚠️ 通信开销可能超过收益

# 推理 (Decode阶段):
# - 每次生成1个token, S=1
# ❌ 序列并行无意义
```

#### 超长序列推理

序列并行在超长序列推理时仍然重要：

```python
# Ring Attention + 序列并行
# 处理100K+ token的超长上下文

# 策略:
# 1. 序列切分到多GPU: 每GPU处理12.5K tokens
# 2. Ring方式传递KV Cache
# 3. 重叠计算与通信

# 优势:
# - 突破单GPU显存限制
# - 支持任意长度序列
# - 无需重计算或近似
```

### 7. 实现示例

#### PyTorch实现框架

```python
import torch
import torch.distributed as dist

class SequenceParallel:
    def __init__(self, world_size, rank):
        self.world_size = world_size
        self.rank = rank
        
    def split_sequence(self, tensor):
        """将序列切分到多个GPU"""
        # tensor: [B, S, H]
        B, S, H = tensor.shape
        assert S % self.world_size == 0
        
        seq_len_per_rank = S // self.world_size
        start = self.rank * seq_len_per_rank
        end = start + seq_len_per_rank
        
        return tensor[:, start:end, :]  # [B, S/N, H]
    
    def all_gather_sequence(self, tensor):
        """收集完整序列"""
        # tensor: [B, S/N, H]
        gathered = [torch.empty_like(tensor) for _ in range(self.world_size)]
        dist.all_gather(gathered, tensor)
        return torch.cat(gathered, dim=1)  # [B, S, H]
    
    def layernorm_sp(self, x, weight, bias):
        """序列并行的LayerNorm"""
        # x: [B, S/N, H] 已经是序列分片
        # 直接在本地计算 (LayerNorm在hidden维度)
        return F.layer_norm(x, (x.size(-1),), weight, bias)
    
    def attention_sp(self, Q, K, V):
        """序列并行的Attention (需要All-Gather)"""
        # Q, K, V: [B, S/N, H]
        
        # 收集完整的K, V用于注意力计算
        K_full = self.all_gather_sequence(K)  # [B, S, H]
        V_full = self.all_gather_sequence(V)  # [B, S, H]
        
        # 计算注意力 (Q仍然是分片的)
        scores = torch.matmul(Q, K_full.transpose(-2, -1))  # [B, S/N, S]
        attn = F.softmax(scores, dim=-1)
        output = torch.matmul(attn, V_full)  # [B, S/N, H]
        
        return output  # 输出仍保持序列分片
```

### 8. 总结

#### 关键特点

1. **切分维度**: 序列长度维度（不同于TP的特征维度）
2. **主要优势**: 大幅降低激活显存（降至1/N）
3. **适用算子**: LayerNorm、Dropout等逐元素操作
4. **需要结合**: 通常与张量并行配合使用
5. **通信开销**: All-to-All转换布局（相比TP更频繁）

#### 适用条件

- ✅ 超长序列训练（2K+ tokens）
- ✅ 与张量并行结合使用
- ✅ 激活显存是主要瓶颈
- ⚠️ 推理场景需权衡通信开销
- ❌ 短序列或Decode阶段收益有限

序列并行是长序列场景下的重要优化手段，特别是在现代大模型追求更长上下文窗口的趋势下，其重要性日益凸显。


---

## 相关笔记
<!-- 自动生成 -->

- [序列并行如何沿序列维度切分？](notes/熟悉大语言模型推理优化-技术层次/序列并行如何沿序列维度切分？.md) - 相似度: 33% | 标签: 熟悉大语言模型推理优化-技术层次, 熟悉大语言模型推理优化-技术层次/序列并行如何沿序列维度切分？.md
- [如何减少张量并行中的通信开销？](notes/熟悉大语言模型推理优化-技术层次/如何减少张量并行中的通信开销？.md) - 相似度: 31% | 标签: 熟悉大语言模型推理优化-技术层次, 熟悉大语言模型推理优化-技术层次/如何减少张量并行中的通信开销？.md
- [序列并行如何与张量并行结合使用？](notes/熟悉大语言模型推理优化-技术层次/序列并行如何与张量并行结合使用？.md) - 相似度: 31% | 标签: 熟悉大语言模型推理优化-技术层次, 熟悉大语言模型推理优化-技术层次/序列并行如何与张量并行结合使用？.md

