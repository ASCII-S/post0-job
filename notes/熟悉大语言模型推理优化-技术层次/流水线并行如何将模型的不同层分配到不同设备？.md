---
created: '2025-10-19'
last_reviewed: null
next_review: '2025-10-19'
review_count: 0
difficulty: medium
mastery_level: 0.0
tags:
- 熟悉大语言模型推理优化-技术层次
- 熟悉大语言模型推理优化-技术层次/流水线并行如何将模型的不同层分配到不同设备？.md
related_outlines: []
---
# 流水线并行如何将模型的不同层分配到不同设备？

## 面试标准答案

流水线并行将模型按层切分成多个stage，每个stage分配到不同的GPU上。例如96层的GPT-3可以分成8个stage，每个stage包含12层。数据像流水线一样在各stage间依次流动，前一个stage处理完一个micro-batch后，将中间激活传递给下一个stage。关键是要平衡各stage的计算量，避免某些stage成为瓶颈，同时通过micro-batch流水线化来减少GPU空闲时间（气泡）。

---

## 详细讲解

### 1. 流水线并行的基本概念

#### 1.1 原理

将深度神经网络按层划分成多个连续的stage（阶段），每个stage部署在不同的设备上：

```
模型: Layer 1 → Layer 2 → ... → Layer N

流水线划分 (4-way PP):
GPU 0: Layers 1-N/4
GPU 1: Layers (N/4+1)-N/2
GPU 2: Layers (N/2+1)-3N/4
GPU 3: Layers (3N/4+1)-N
```

#### 1.2 数据流动

```
时间 →

GPU0: [Batch1] →
GPU1:           [Batch1] →
GPU2:                     [Batch1] →
GPU3:                               [Batch1]
```

### 2. Stage划分策略

#### 2.1 均匀划分

最简单的方法：每个stage包含相同数量的层

```python
def uniform_partition(num_layers, num_stages):
    """
    均匀划分层到各stage
    """
    layers_per_stage = num_layers // num_stages
    partition = []
    
    for stage in range(num_stages):
        start = stage * layers_per_stage
        end = (stage + 1) * layers_per_stage
        if stage == num_stages - 1:
            end = num_layers  # 最后一个stage包含剩余所有层
        partition.append((start, end))
    
    return partition

# 示例: GPT-3 (96层，8 stages)
# [(0,12), (12,24), (24,36), (36,48), (48,60), (60,72), (72,84), (84,96)]
```

**优点**：
- 简单直观
- 易于实现

**缺点**：
- 忽略不同层的计算量差异
- 可能导致负载不均衡

#### 2.2 基于计算量的划分

根据每层的实际计算时间进行划分：

```python
def compute_balanced_partition(layer_costs, num_stages):
    """
    layer_costs: 每层的计算时间（通过profiling获得）
    num_stages: stage数量
    """
    total_cost = sum(layer_costs)
    target_cost_per_stage = total_cost / num_stages
    
    partition = []
    current_stage_start = 0
    current_stage_cost = 0
    
    for layer_idx, cost in enumerate(layer_costs):
        current_stage_cost += cost
        
        # 如果当前stage的成本接近目标，开始新stage
        if current_stage_cost >= target_cost_per_stage:
            partition.append((current_stage_start, layer_idx + 1))
            current_stage_start = layer_idx + 1
            current_stage_cost = 0
    
    # 最后一个stage
    if current_stage_start < len(layer_costs):
        partition.append((current_stage_start, len(layer_costs)))
    
    return partition
```

**示例计算**（GPT-3）：
```python
# 假设profiling结果（ms）
layer_costs = [
    15,  # Layer 1: Embedding (较快)
    20, 20, 20, ...,  # 中间层
    25   # 最后一层: LM head (较慢)
]

# 基于成本划分可能得到:
# Stage 0: 13 layers (embedding + 12层)
# Stage 1-6: 12 layers each
# Stage 7: 11 layers (包含LM head)
```

#### 2.3 考虑内存的划分

某些层占用更多显存（如embedding），需要特殊处理：

```python
def memory_aware_partition(layer_params, layer_activations, 
                           num_stages, memory_limit_per_gpu):
    """
    考虑参数量和激活显存的划分
    """
    partition = []
    current_stage = []
    current_memory = 0
    
    for layer_idx, (params, act_mem) in enumerate(
            zip(layer_params, layer_activations)):
        layer_memory = params + act_mem
        
        if current_memory + layer_memory > memory_limit_per_gpu:
            # 当前stage已满，开始新stage
            partition.append(current_stage)
            current_stage = [layer_idx]
            current_memory = layer_memory
        else:
            current_stage.append(layer_idx)
            current_memory += layer_memory
    
    if current_stage:
        partition.append(current_stage)
    
    return partition
```

### 3. Micro-batch流水线化

#### 3.1 为什么需要Micro-batch

**问题**：简单的流水线会产生大量空闲时间（气泡）

```
无Micro-batch:
GPU0: [====Batch1====]                      (空闲)
GPU1:                 [====Batch1====]      (空闲)
GPU2:                                 [====Batch1====]
GPU3:                                                 [====Batch1====]

气泡时间 = 3 × T_stage (75%空闲！)
```

**解决**：将batch分成多个micro-batch

```
有Micro-batch (4个):
GPU0: [B1-m1][B1-m2][B1-m3][B1-m4]
GPU1:        [B1-m1][B1-m2][B1-m3][B1-m4]
GPU2:               [B1-m1][B1-m2][B1-m3][B1-m4]
GPU3:                      [B1-m1][B1-m2][B1-m3][B1-m4]

气泡减少!
```

#### 3.2 Micro-batch大小选择

```python
def calculate_optimal_microbatch_size(global_batch_size, 
                                       num_microbatches,
                                       num_stages):
    """
    global_batch_size: 总batch大小
    num_microbatches: micro-batch数量
    """
    microbatch_size = global_batch_size // num_microbatches
    
    # 经验法则: num_microbatches >= num_stages
    # 更多的micro-batch减少气泡，但增加通信次数
    
    return microbatch_size

# 示例
global_batch = 32
num_stages = 8
num_microbatches = 32  # 通常选择 4×num_stages

microbatch_size = 32 // 32 = 1  # 每个micro-batch只有1个样本
```

### 4. 具体划分示例

#### 4.1 GPT-3 (175B参数)

```python
# 模型配置
num_layers = 96
hidden_size = 12288
num_attention_heads = 96

# 8-way流水线并行
num_stages = 8
layers_per_stage = 96 // 8 = 12

partition_plan = {
    'Stage 0 (GPU 0)': {
        'layers': 'Embedding + Layers 1-11',
        'params': '~22B',
        'description': 'Embedding较大，所以层数稍少'
    },
    'Stage 1-6 (GPU 1-6)': {
        'layers': 'Layers 12-83 (每个stage 12层)',
        'params': '~22B each',
        'description': '中间层，均匀分配'
    },
    'Stage 7 (GPU 7)': {
        'layers': 'Layers 84-96 + LM Head',
        'params': '~23B',
        'description': 'LM Head较大'
    }
}
```

#### 4.2 LLaMA-65B

```python
# 4-way流水线并行
num_layers = 80
num_stages = 4

partition = {
    'Stage 0': [0, 20],   # Layers 0-19
    'Stage 1': [20, 40],  # Layers 20-39
    'Stage 2': [40, 60],  # Layers 40-59
    'Stage 3': [60, 80],  # Layers 60-79
}

# 每个stage约16B参数
```

### 5. 实现细节

#### 5.1 模型划分代码

```python
class PipelineModel(nn.Module):
    def __init__(self, full_model, partition, rank):
        super().__init__()
        self.rank = rank
        self.start_layer, self.end_layer = partition[rank]
        
        # 只保留当前stage的层
        self.layers = nn.ModuleList([
            full_model.layers[i] 
            for i in range(self.start_layer, self.end_layer)
        ])
        
        # 第一个stage需要embedding
        if rank == 0:
            self.embedding = full_model.embedding
        
        # 最后一个stage需要输出层
        if rank == len(partition) - 1:
            self.lm_head = full_model.lm_head
    
    def forward(self, x):
        # 第一个stage: embedding
        if self.rank == 0:
            x = self.embedding(x)
        
        # 中间层
        for layer in self.layers:
            x = layer(x)
        
        # 最后一个stage: 输出
        if self.rank == len(partition) - 1:
            x = self.lm_head(x)
        
        return x
```

#### 5.2 数据传输

```python
def pipeline_forward(input_ids, microbatch_size):
    """
    流水线前向传播
    """
    # 分割成micro-batches
    microbatches = input_ids.split(microbatch_size)
    
    for microbatch in microbatches:
        # 第一个stage处理
        if rank == 0:
            activations = stage_0_model(microbatch)
            # 发送到下一个stage
            send_tensor(activations, dst=rank+1)
        
        # 中间stage
        elif 0 < rank < num_stages - 1:
            # 接收上一个stage的激活
            activations = recv_tensor(src=rank-1)
            # 处理
            activations = current_stage_model(activations)
            # 发送到下一个stage
            send_tensor(activations, dst=rank+1)
        
        # 最后一个stage
        else:
            activations = recv_tensor(src=rank-1)
            output = final_stage_model(activations)
            # 计算损失
```

### 6. 负载均衡考虑

#### 6.1 不同层的计算差异

```python
# Transformer层的计算时间构成
attention_time = 2 * B * S^2 * H / FLOPS  # 注意力
ffn_time = 8 * B * S * H^2 / FLOPS        # FFN

# 对于长序列，attention可能更慢
# 对于短序列，FFN占主导

# 需要根据实际workload profiling
```

#### 6.2 动态调整

```python
# 在实际运行中监控各stage的耗时
stage_times = profile_pipeline_stages()

# 根据测量结果重新划分
if max(stage_times) / min(stage_times) > 1.2:  # 不平衡超过20%
    print("Warning: Load imbalance detected!")
    # 可以重新划分或调整micro-batch
```

### 7. 流水线调度策略

#### 7.1 GPipe调度

```
Fill阶段: 填满流水线
Steady阶段: 稳定运行
Drain阶段: 排空流水线

时间 →
GPU0: [F1][F2][F3][F4][S1][S2]...[D1]
GPU1:     [F1][F2][F3][F4][S1]...[D1][D2]
GPU2:         [F1][F2][F3][F4]...[D1][D2][D3]
GPU3:             [F1][F2][F3]...[D1][D2][D3][D4]
```

#### 7.2 PipeDream调度

使用1F1B (One Forward One Backward)策略减少显存占用

### 8. 跨节点部署

#### 8.1 节点间划分

```python
# 例如: 8个节点，每节点8 GPU
# 方案1: 流水线跨节点
PP_degree = 8  # 8个stage，每个stage用一个节点
TP_degree = 8  # 节点内8卡做张量并行

# 方案2: 流水线节点内
PP_degree = 8  # 节点内流水线
DP_degree = 8  # 8个节点做数据并行
```

#### 8.2 通信开销

```
节点内（NVLink）: 激活传输快
节点间（InfiniBand）: 激活传输慢

策略: 在stage边界最小化激活大小
```

### 9. 实际部署示例

#### 9.1 Megatron-LM配置

```bash
# 启动脚本
python -m torch.distributed.launch \
  --nproc_per_node 8 \
  --nnodes 4 \
  pretrain_gpt.py \
  --tensor-model-parallel-size 8 \  # 节点内TP
  --pipeline-model-parallel-size 4 \  # 跨节点PP
  --num-layers 96 \
  --hidden-size 12288 \
  --num-attention-heads 96 \
  --micro-batch-size 1 \
  --global-batch-size 32
```

#### 9.2 DeepSpeed配置

```json
{
  "pipeline": {
    "stages": 4,
    "partition_method": "parameters",
    "activation_checkpointing": true
  },
  "train_micro_batch_size_per_gpu": 1,
  "gradient_accumulation_steps": 8
}
```

### 10. 优化建议

**Stage划分**：
1. ✅ Profiling确定计算时间
2. ✅ 考虑显存占用
3. ✅ 平衡各stage负载
4. ✅ 特殊处理embedding和lm_head

**Micro-batch**：
1. ✅ 数量 ≥ 4 × num_stages
2. ✅ 平衡气泡时间和通信开销
3. ✅ 考虑显存限制

**通信**：
1. ✅ 流水线stage间用点对点通信
2. ✅ 尽量将stage分配到高速互连的GPU
3. ✅ 压缩激活值（混合精度）

流水线并行通过合理的层划分和micro-batch调度，可以高效地将大模型分布到多个GPU，是跨节点扩展的关键技术。


---

## 相关笔记
<!-- 自动生成 -->

- [MoE模型中的专家如何分布到多个设备？](notes/熟悉大语言模型推理优化-技术层次/MoE模型中的专家如何分布到多个设备？.md) - 相似度: 31% | 标签: 熟悉大语言模型推理优化-技术层次, 熟悉大语言模型推理优化-技术层次/MoE模型中的专家如何分布到多个设备？.md

