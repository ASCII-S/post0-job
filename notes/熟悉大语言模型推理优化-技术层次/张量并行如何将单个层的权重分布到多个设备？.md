---
created: '2025-10-19'
last_reviewed: null
next_review: '2025-10-19'
review_count: 0
difficulty: medium
mastery_level: 0.0
tags:
- 熟悉大语言模型推理优化-技术层次
- 熟悉大语言模型推理优化-技术层次/张量并行如何将单个层的权重分布到多个设备？.md
related_outlines: []
---
# 张量并行如何将单个层的权重分布到多个设备？

## 面试标准答案

张量并行通过将单个层的权重矩阵沿着某个维度切分到多个设备上，每个设备只持有权重的一部分。在前向传播时，各设备并行计算各自部分的矩阵乘法，然后通过集合通信操作（如All-Reduce）汇总结果。这种方法使得单个层的计算和存储可以分散到多个GPU，突破单卡显存限制，同时保持计算并行性。

---

## 详细讲解

### 1. 张量并行的核心思想

张量并行（Tensor Parallelism）是一种模型并行策略，它将单个神经网络层内部的权重张量切分到多个设备上。与流水线并行按层切分不同，张量并行在层内进行切分，使得一个层的计算可以同时在多个GPU上进行。

**基本原理**：
- 将权重矩阵 $W \in \mathbb{R}^{m \times n}$ 切分为多个子矩阵
- 每个设备持有权重的一个分片，负责计算部分结果
- 通过通信原语合并各设备的计算结果

### 2. 权重切分方式

#### 2.1 列切分（Column Parallelism）

将权重矩阵 $W$ 按列切分为 $[W_1, W_2, ..., W_N]$，其中 N 是并行度。

对于矩阵乘法 $Y = XW$：
- 输入 $X$ 在所有设备上复制
- 每个设备 $i$ 计算 $Y_i = XW_i$
- 输出 $Y = [Y_1, Y_2, ..., Y_N]$ 在各设备上是分片的

**示例**（2路并行）：
```
设备1: Y₁ = X · W₁  (输出维度的前半部分)
设备2: Y₂ = X · W₂  (输出维度的后半部分)
```

**特点**：
- 输出是分片的，需要后续操作聚合或继续分片计算
- 适用于 Transformer 的 QKV 投影、FFN 的第一层

#### 2.2 行切分（Row Parallelism）

将权重矩阵 $W$ 按行切分为 $\begin{bmatrix} W_1 \\ W_2 \\ ... \\ W_N \end{bmatrix}$。

对于矩阵乘法 $Y = XW$：
- 输入 $X$ 也需要按列切分为 $[X_1, X_2, ..., X_N]$
- 每个设备 $i$ 计算 $Y_i = X_i W_i$
- 需要 All-Reduce 将各部分结果求和：$Y = \sum_i Y_i$

**示例**（2路并行）：
```
设备1: Y₁ = X₁ · W₁
设备2: Y₂ = X₂ · W₂
最终: Y = Y₁ + Y₂  (通过 All-Reduce)
```

**特点**：
- 输入是分片的，输出需要 All-Reduce 聚合
- 适用于 FFN 的第二层、输出投影

### 3. 完整的前向传播流程

以两层 MLP 为例（$Y = \text{GeLU}(XW_1)W_2$）：

**第一层（列并行）**：
1. 输入 $X$ 在所有设备上复制
2. 设备 $i$ 计算 $Z_i = XW_{1,i}$
3. 激活函数在各设备独立应用：$A_i = \text{GeLU}(Z_i)$
4. 输出 $A = [A_1, A_2, ..., A_N]$ 保持分片

**第二层（行并行）**：
1. 输入已经是分片的 $A_i$
2. 设备 $i$ 计算 $Y_i = A_i W_{2,i}$
3. All-Reduce 汇总：$Y = \sum_i Y_i$

**通信次数**：每两层只需一次 All-Reduce，大大减少通信开销。

### 4. 显存分配

假设模型参数量为 P，并行度为 N：

**权重显存**：
- 每个设备持有 $P/N$ 的参数
- 节省显存：$\frac{N-1}{N} \times P$

**激活显存**：
- 取决于具体的切分策略
- 列并行：激活在输出维度分片
- 行并行：激活在输入维度分片

### 5. 数学表示

对于一般的矩阵乘法 $Y = f(XW + b)$：

**列切分**：
$$Y = [f(XW_1 + b_1), f(XW_2 + b_2), ..., f(XW_N + b_N)]$$

**行切分**：
$$Y = f(\sum_{i=1}^N X_i W_i + b)$$

### 6. 实际应用场景

**适用情况**：
- 单层参数量超过单卡显存
- 需要在层内实现细粒度并行
- 节点内多卡高速互连（NVLink）

**不适用情况**：
- 跨节点通信带宽有限
- 模型层数多但单层参数少
- Batch size 很小导致通信占比高

### 7. 优势与限制

**优势**：
- 突破单卡显存限制
- 可以并行化非常大的单层
- 与其他并行策略正交，可以组合使用

**限制**：
- 需要高速互连（NVLink/InfiniBand）
- 通信开销随并行度增加
- 代码实现复杂度较高
- 需要模型代码适配

### 8. 关键技术要点

1. **切分维度选择**：根据矩阵形状和后续操作选择行切分或列切分
2. **通信优化**：合并通信操作，减少同步点
3. **负载均衡**：确保各设备计算量相近
4. **显存管理**：合理分配权重、激活、梯度显存

张量并行是大模型分布式推理的核心技术之一，理解其权重切分机制对于高效部署大规模模型至关重要。


---

## 相关笔记
<!-- 自动生成 -->

- [列并行和行并行的区别是什么？](notes/熟悉大语言模型推理优化-技术层次/列并行和行并行的区别是什么？.md) - 相似度: 36% | 标签: 熟悉大语言模型推理优化-技术层次, 熟悉大语言模型推理优化-技术层次/列并行和行并行的区别是什么？.md
- [MoE模型中的专家如何分布到多个设备？](notes/熟悉大语言模型推理优化-技术层次/MoE模型中的专家如何分布到多个设备？.md) - 相似度: 31% | 标签: 熟悉大语言模型推理优化-技术层次, 熟悉大语言模型推理优化-技术层次/MoE模型中的专家如何分布到多个设备？.md
- [Megatron-LM如何实现Transformer的张量并行？](notes/熟悉大语言模型推理优化-技术层次/Megatron-LM如何实现Transformer的张量并行？.md) - 相似度: 31% | 标签: 熟悉大语言模型推理优化-技术层次, 熟悉大语言模型推理优化-技术层次/Megatron-LM如何实现Transformer的张量并行？.md

