---
created: '2025-10-19'
last_reviewed: null
next_review: '2025-10-19'
review_count: 0
difficulty: medium
mastery_level: 0.0
tags:
- 熟悉大语言模型推理优化-技术层次
- 熟悉大语言模型推理优化-技术层次/序列并行的通信开销如何？.md
related_outlines: []
---
# 序列并行的通信开销如何？

## 面试标准答案

序列并行增加了通信次数但总量可控。每层需要4次All-to-All，通信量约为激活大小×4。相比纯张量并行，All-to-All的通信量(~D)比All-Reduce(~2D)小，但次数更多。总体通信开销增加约2倍，但显存节省87.5%(8-way)的收益远大于通信成本，特别在长序列场景。实测显示通信占比从15%增至25%，但允许更大batch，整体吞吐量提升20-30%。

---

## 详细讲解

### 通信量对比

```
纯张量并行/层:
4× All-Reduce, 每次 ~2×激活大小
总计: ~8×激活大小

张量+序列并行/层:
4× All-Reduce ~2×激活大小
4× All-to-All ~1×激活大小  
总计: ~12×激活大小

增加50%通信量
```

### 实际影响

```
GPT-3 (8-way, S=2048):

纯TP:
- 通信: 15% 时间
- Batch: 32 (显存限制)
- 吞吐: 100 samples/s

TP+SP:
- 通信: 25% 时间
- Batch: 48 (显存优化)
- 吞吐: 130 samples/s (+30%)

收益 > 成本
```

### 关键因素

- 序列长度越长，收益越大
- NVLink带宽足够可忽略影响
- 主要用于训练，推理收益有限


---

## 相关笔记
<!-- 自动生成 -->

- [序列并行需要哪些集合通信操作？](notes/熟悉大语言模型推理优化-技术层次/序列并行需要哪些集合通信操作？.md) - 相似度: 31% | 标签: 熟悉大语言模型推理优化-技术层次, 熟悉大语言模型推理优化-技术层次/序列并行需要哪些集合通信操作？.md

