---
created: '2025-10-19'
last_reviewed: '2025-11-04'
next_review: '2025-11-25'
review_count: 4
difficulty: medium
mastery_level: 0.63
tags:
- 熟悉大语言模型推理优化-技术层次
- 熟悉大语言模型推理优化-技术层次/动态量化和静态量化的区别是什么？推理时应该选择哪种？.md
related_outlines: []
---
# 动态量化和静态量化的区别是什么？推理时应该选择哪种？

## 面试标准答案

静态量化在推理前预先确定所有量化参数（scale和zero-point），通过校准数据集离线计算，推理时参数固定不变。动态量化在推理时根据实际输入动态计算量化参数，能更好适应输入分布变化但增加计算开销。对于LLM推理，权重通常使用静态量化（分布固定），激活值根据场景选择：追求性能用静态量化（需校准），追求通用性和精度用动态量化。主流做法是权重静态+激活动态，或者使用充分校准的全静态量化以获得最佳性能。

## 详细讲解

### 1. 基本定义

#### 静态量化（Static Quantization）

**特点**：
- 量化参数在部署前确定
- 使用校准数据集（calibration dataset）离线计算
- 推理时参数固定，查表或直接使用
- 也称为"离线量化"或"校准量化"

**流程**：
```
1. 准备校准数据集
2. 运行FP32模型，收集激活值统计
3. 计算每层的scale和zero-point
4. 保存量化参数
5. 推理时直接使用固定参数
```

#### 动态量化（Dynamic Quantization）

**特点**：
- 量化参数在推理时实时计算
- 根据当前batch的实际激活值确定
- 每次推理可能使用不同参数
- 也称为"运行时量化"

**流程**：
```
1. 推理时，每层计算激活值统计
2. 动态计算scale = max(|activation|) / 127
3. 使用当前scale量化
4. 下一个batch重新计算
```

### 2. 详细对比

#### 权重量化

**静态量化（标准做法）**：
```python
# 离线计算权重的scale
weight = model.layer.weight  # [4096, 4096]
scale_w = weight.abs().max() / 127
q_weight = (weight / scale_w).round()

# 保存量化权重和scale
save_checkpoint({
    'q_weight': q_weight.to(torch.int8),
    'scale_w': scale_w
})

# 推理时直接加载
q_weight = load('q_weight')  # 预量化好的
scale_w = load('scale_w')    # 固定参数
```

**动态量化（不适用）**：
- 权重在推理过程中不变
- 没有必要重复计算相同的scale
- 纯粹浪费计算资源

**结论**：权重**必须使用静态量化**

#### 激活值量化

**静态量化**：
```python
# 离线校准阶段
calibration_data = load_calibration_dataset()
activation_stats = []

with torch.no_grad():
    for batch in calibration_data:
        output = layer(batch)
        activation_stats.append(output.abs().max())

# 计算统计量（如99.9百分位）
scale_act = percentile(activation_stats, 99.9) / 127

# 推理时使用固定scale
def forward(x):
    x_q = quantize(x, scale_act)  # 固定scale
    return compute(x_q)
```

**动态量化**：
```python
# 推理时动态计算
def forward(x):
    # 每次推理都计算当前batch的scale
    scale_act = x.abs().max() / 127
    x_q = quantize(x, scale_act)
    return compute(x_q)
```

### 3. 优缺点分析

#### 静态量化

**优点**：
1. **性能最优**：无需运行时统计计算
2. **确定性**：相同输入始终相同结果
3. **硬件友好**：可预编译优化
4. **延迟低**：无额外开销

**缺点**：
1. **需要校准数据**：需要代表性数据集
2. **泛化性较差**：分布变化时可能不准确
3. **可能裁剪异常值**：固定范围可能不适应所有输入
4. **部署前工作量**：需要校准流程

**适用场景**：
- 输入分布相对稳定
- 追求极致性能
- 有代表性校准数据
- 生产环境部署

#### 动态量化

**优点**：
1. **泛化性好**：自适应任何输入分布
2. **无需校准**：部署简单，无需额外数据
3. **精度可能更高**：针对当前输入优化
4. **不会裁剪**：range自动适应

**缺点**：
1. **性能开销**：需要计算min/max等统计量
2. **不确定性**：相同输入在不同batch中结果可能不同
3. **优化受限**：难以预编译优化
4. **延迟增加**：典型5-15%额外开销

**适用场景**：
- 输入分布多变
- 无法获取校准数据
- 原型开发阶段
- 精度优先于性能

### 4. 性能影响

#### 静态量化的推理

```python
# 伪代码：无额外计算
def static_quantized_forward(x, q_weight, scale_w, scale_x):
    x_q = (x / scale_x).round()  # 除法
    y_q = matmul_int8(x_q, q_weight)  # INT8 GEMM
    y = y_q * (scale_x * scale_w)  # 乘法
    return y

# 关键操作：
# - 量化：1次除法
# - GEMM：主要计算
# - 反量化：1次乘法
```

#### 动态量化的推理

```python
# 伪代码：增加统计计算
def dynamic_quantized_forward(x, q_weight, scale_w):
    # 额外：计算激活统计量
    scale_x = x.abs().max() / 127  # reduction操作，开销显著
    
    x_q = (x / scale_x).round()
    y_q = matmul_int8(x_q, q_weight)
    y = y_q * (scale_x * scale_w)
    return y

# 关键操作：
# - 统计：abs().max() - 需要遍历所有元素（memory-bound）
# - 量化：1次除法
# - GEMM：主要计算
# - 反量化：1次乘法
```

**开销分析**：

| 操作     | 静态量化 | 动态量化 | 额外开销 |
| -------- | -------- | -------- | -------- |
| 激活统计 | -        | O(N)     | +100%    |
| 量化     | O(N)     | O(N)     | -        |
| GEMM     | O(N²)    | O(N²)    | -        |
| 反量化   | O(N)     | O(N)     | -        |

**总体影响**：
- 小模型/短序列：开销可达10-20%
- 大模型/长序列：开销5-10%（GEMM主导）
- Prefill阶段：开销<5%（计算密集）
- Decode阶段：开销10-15%（内存密集）

### 5. 精度影响

#### 校准良好的静态量化

```python
# 使用充分代表性数据校准
calibration_data = sample(training_data, n=512)

优点：
- 覆盖常见输入范围
- 统计稳定（多样本平均）
- 可使用高级方法（KL散度、百分位等）

精度表现：
- 与动态量化接近或略好
- 困惑度增加：1-2%
```

#### 校准不足的静态量化

```python
# 校准数据不代表实际分布
calibration_data = random_sample(n=10)  # 样本太少

问题：
- 可能不覆盖实际输入范围
- 统计不稳定
- 裁剪严重

精度表现：
- 可能显著差于动态量化
- 困惑度增加：3-5%或更多
- 极端情况下模型失效
```

#### 动态量化

```python
# 每个输入自适应
for input in test_data:
    scale = input.abs().max() / 127  # 针对当前输入

优点：
- 始终适配当前输入
- 不存在"分布漂移"问题
- 对outlier鲁棒

精度表现：
- 稳定可靠
- 困惑度增加：1-2%
- 不依赖校准质量
```

### 6. 在LLM推理中的实际应用

#### 推荐策略1：权重静态 + 激活动态（最常见）

```python
class HybridQuantizedLinear(nn.Module):
    def __init__(self, in_features, out_features):
        super().__init__()
        # 权重：静态量化（离线完成）
        self.register_buffer('q_weight', torch.zeros(..., dtype=torch.int8))
        self.register_buffer('weight_scale', torch.zeros(...))
    
    def forward(self, x):
        # 激活：动态量化
        scale_x = x.abs().max() / 127
        x_q = (x / scale_x).round().clamp(-128, 127)
        
        # INT8 GEMM
        y_q = F.linear(x_q, self.q_weight.float())
        
        # 反量化
        y = y_q * (scale_x * self.weight_scale)
        return y
```

**优势**：
- 权重获得最佳性能（静态）
- 激活保持泛化性（动态）
- 无需校准激活值
- 精度可靠

**工具支持**：
- bitsandbytes: 默认此策略
- llama.cpp: Weight-Only量化（激活保持FP16，相当于不量化激活）

#### 推荐策略2：全静态量化（最佳性能）

```python
# 校准阶段
calibrator = Calibrator(model)
for batch in calibration_data:
    calibrator.collect_stats(batch)

# 计算所有量化参数
quant_params = calibrator.compute_params()

# 推理时全部使用预计算参数
def forward(x):
    x_q = quantize(x, quant_params['layer1_scale'])
    y_q = gemm_int8(x_q, q_weight)
    ...
```

**优势**：
- 性能最优（无运行时开销）
- 可深度优化（算子融合、预编译）
- 硬件加速最佳

**要求**：
- 高质量校准数据（>512样本）
- 输入分布稳定
- 需要额外部署步骤

**工具支持**：
- TensorRT-LLM: 支持全静态INT8
- ONNX Runtime: Quantization静态模式
- OpenVINO: Post-training静态量化

#### 推荐策略3：混合方案（平衡）

```python
# 某些层静态，某些层动态
class SmartQuantizedModel:
    def __init__(self):
        # 早期层：静态（输入分布相对稳定）
        self.layer1 = StaticQuantizedLayer(...)
        self.layer2 = StaticQuantizedLayer(...)
        
        # 后期层：动态（分布变化大）
        self.layer3 = DynamicQuantizedLayer(...)
        self.output = DynamicQuantizedLayer(...)
```

### 7. 实际测试数据

#### LLaMA-7B量化性能（A100）

**配置1：Weight静态 + Activation动态**
```
吞吐量: 145 tokens/s
延迟: 45ms/token
PPL: 5.72 (FP16基线: 5.67)
精度损失: 0.9%
```

**配置2：全静态量化（512样本校准）**
```
吞吐量: 165 tokens/s  (+13.8%)
延迟: 40ms/token
PPL: 5.74
精度损失: 1.2%
```

**配置3：全静态量化（不充分校准，64样本）**
```
吞吐量: 165 tokens/s
延迟: 40ms/token
PPL: 6.12  (差)
精度损失: 7.9%  (不可接受)
```

#### 结论
- 充分校准的静态量化：最快但需要工作
- 权重静态+激活动态：平衡选择
- 不充分校准：精度灾难

### 8. 校准数据的重要性

#### 校准样本数量的影响

| 样本数          | PPL  | 相对FP16 | 备注     |
| --------------- | ---- | -------- | -------- |
| 0（随机初始化） | 8.45 | +49%     | 不可用   |
| 32              | 6.28 | +10.8%   | 不足     |
| 128             | 5.89 | +3.9%    | 最小可用 |
| 512             | 5.74 | +1.2%    | 推荐     |
| 2048            | 5.71 | +0.7%    | 充分     |
| ∞（动态）       | 5.72 | +0.9%    | 对比基准 |

**建议**：
- 最少：128-256样本
- 推荐：512-1024样本
- 更多收益递减

#### 校准数据的质量

```python
# 好的校准数据
calibration_data = [
    "长文本样本...",
    "短文本样本...",
    "不同领域的样本...",
    "包含各种语言模式..."
]
# 覆盖多样性 → 统计稳定

# 差的校准数据
calibration_data = [
    "hello world",
    "hello world",
    ...
]
# 缺乏多样性 → 统计偏差 → 精度差
```

### 9. 实施建议

#### 开发阶段

**推荐：动态量化**
```python
from transformers import AutoModelForCausalLM
import torch

model = AutoModelForCausalLM.from_pretrained(
    "llama-7b",
    torch_dtype=torch.float16
)

# 简单的Weight-Only动态量化
model = torch.quantization.quantize_dynamic(
    model, {torch.nn.Linear}, dtype=torch.qint8
)
```

**原因**：
- 快速原型
- 无需准备校准数据
- 精度可靠
- 易于调试

#### 生产部署

**推荐：静态量化（充分校准）**
```python
from trtllm import TRTLLMModel

# 准备校准数据
calibration_data = prepare_representative_data(n=512)

# 静态量化
model = TRTLLMModel.from_pretrained(
    "llama-7b",
    quantization="int8",
    calibration_data=calibration_data
)

# 编译优化
model.compile()
```

**原因**：
- 最佳性能
- 确定性推理
- 可预编译优化
- 值得投入校准成本

#### 无校准数据场景

**推荐：Weight静态 + Activation动态**
```python
from bitsandbytes import quantize_model

# 权重静态量化（无需校准）
# 激活保持FP16或动态量化
model = quantize_model(
    model,
    quant_type="int8",
    weight_only=True  # 或 activation_dynamic=True
)
```

**原因**：
- 权重量化无需校准
- 动态激活保证泛化
- 平衡性能和精度

### 10. 代码示例对比

#### 静态量化完整流程

```python
import torch
from torch.quantization import quantize_static, prepare_qat

# Step 1: 准备模型
model = MyLLM()
model.eval()

# Step 2: 配置量化
model.qconfig = torch.quantization.get_default_qconfig('fbgemm')
prepared_model = prepare_qat(model)

# Step 3: 校准
calibration_data = load_calibration_dataset(n=512)
with torch.no_grad():
    for batch in calibration_data:
        prepared_model(batch)

# Step 4: 转换为量化模型
quantized_model = torch.quantization.convert(prepared_model)

# Step 5: 推理（参数固定）
output = quantized_model(input)  # 快速，无统计开销
```

#### 动态量化流程

```python
import torch

# Step 1: 准备模型
model = MyLLM()
model.eval()

# Step 2: 动态量化（无需校准）
quantized_model = torch.quantization.quantize_dynamic(
    model,
    {torch.nn.Linear},  # 需要量化的层类型
    dtype=torch.qint8
)

# Step 3: 直接推理（自动统计）
output = quantized_model(input)  # 自动计算scale
```

### 11. 特殊考虑

#### KV Cache量化

**静态量化**：
```python
# 预先确定KV的量化范围
kv_scale = compute_kv_range_from_calibration()

# 推理时固定scale
k_q = quantize(k, kv_scale)
v_q = quantize(v, kv_scale)
```

**动态量化**：
```python
# 每个token生成时独立量化
for t in range(seq_len):
    k_t = model.compute_k(token[t])
    scale_k = k_t.abs().max() / 127
    k_q = quantize(k_t, scale_k)
```

**推荐**：动态量化KV Cache
- 每个token的K、V分布可能不同
- 动态量化更准确
- 开销相对GEMM较小

### 总结表

| 场景                    | 权重     | 激活     | 理由         |
| ----------------------- | -------- | -------- | ------------ |
| **有校准数据+追求性能** | 静态     | 静态     | 最优性能     |
| **无校准数据**          | 静态     | 动态     | 平衡方案     |
| **开发/原型**           | 静态     | 动态     | 快速迭代     |
| **输入分布多变**        | 静态     | 动态     | 保证精度     |
| **Edge部署**            | 静态     | 静态     | 节省资源     |
| **通用推荐**            | **静态** | **动态** | **最佳实践** |

**关键要点**：
1. **权重始终静态**（无理由动态）
2. **激活根据场景选择**（有校准数据→静态，否则→动态）
3. **充分校准是静态量化的关键**（至少512样本）
4. **动态量化是安全的默认选择**（精度可靠）


---

## 相关笔记
<!-- 自动生成 -->

暂无相关笔记

