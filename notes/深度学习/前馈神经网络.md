---
created: '2025-10-19'
last_reviewed: null
next_review: '2025-10-19'
review_count: 0
difficulty: medium
mastery_level: 0.0
tags:
- 深度学习
- 深度学习/前馈神经网络.md
related_outlines: []
---
# 前馈神经网络

## 面试标准答案（快速版本）

### Q: 什么是前馈神经网络？

**标准答案：**
前馈神经网络（Feedforward Neural Network）是最基本的神经网络结构：
1. **信息流向**：信息从输入层到输出层单向流动，无循环连接
2. **层次结构**：由输入层、隐藏层、输出层组成，层与层之间全连接
3. **函数映射**：学习从输入空间到输出空间的非线性映射关系
4. **基础地位**：是深度学习的基础，其他复杂网络的基本组件

### Q: 前馈神经网络的参数量如何计算？

**标准答案：**
对于全连接层，参数量 = **权重参数 + 偏置参数**
- **权重参数**：当前层神经元数 × 上一层神经元数
- **偏置参数**：当前层神经元数
- **总参数量**：所有层参数之和

例如：输入层100维 → 隐藏层50维 → 输出层10维
- 第一层：100×50 + 50 = 5050
- 第二层：50×10 + 10 = 510  
- 总计：5560个参数

### Q: 网络深度和宽度如何权衡？

**标准答案：**
- **深度优势**：学习层次化特征，表达能力强，参数效率高
- **宽度优势**：并行计算友好，训练稳定，避免梯度消失
- **权衡原则**：一般优先增加深度，深度过深时考虑增加宽度
- **实践指导**：结合任务复杂度、计算资源、训练稳定性综合考虑

### Q: 万能逼近定理的意义和局限性是什么？

**标准答案：**
万能逼近定理指出：只要激活函数满足非线性条件，单隐层前馈神经网络就可以在紧致区间上，以任意精度逼近任意连续函数。这说明神经网络具有理论上的强大表达能力。
它的局限性是：这是一个存在性定理，并没有告诉我们如何训练得到这样的网络；而且在实际中，要精确逼近复杂函数可能需要极其庞大的网络规模，因此深层结构更高效。

### Q: 前馈神经网络中的梯度是什么？

**标准答案：**
在前馈神经网络中，梯度是损失函数相对于网络参数（权重和偏置）的偏导数：

**数学定义：**
- **权重梯度**：$\frac{\partial L}{\partial W^{(l)}}$ - 损失函数对第l层权重矩阵的偏导数
- **偏置梯度**：$\frac{\partial L}{\partial b^{(l)}}$ - 损失函数对第l层偏置向量的偏导数

**物理意义：**
- 梯度指向损失函数增长最快的方向
- 梯度的大小表示参数对损失函数的敏感程度
- 负梯度方向是损失函数下降最快的方向

**在优化中的作用：**
- **参数更新**：$W^{(l)} \leftarrow W^{(l)} - \alpha \frac{\partial L}{\partial W^{(l)}}$
- **学习信号**：梯度告诉我们如何调整参数以减小损失
- **训练核心**：通过反向传播算法高效计算所有参数的梯度

### Q: 梯度有什么用？

**标准答案：**
梯度在前馈神经网络中有以下核心用途：

**1. 参数优化（最重要）**
- **方向指导**：梯度指明了参数更新的方向，负梯度方向是损失函数下降最快的方向
- **更新公式**：$W \leftarrow W - \alpha \nabla_W L$，其中$\alpha$是学习率
- **收敛保证**：通过梯度下降法使网络逐步收敛到最优解

**2. 学习信号传播**
- **误差传播**：梯度将输出层的误差信息传播到各个隐藏层
- **责任分配**：告诉每个参数对总误差的贡献程度
- **层次调整**：不同层根据梯度大小进行相应的参数调整

**3. 训练监控和诊断**
- **训练状态**：梯度大小反映训练的活跃程度
- **梯度消失/爆炸检测**：通过监控梯度范数识别训练问题
- **收敛判断**：梯度趋于零时表明接近局部最优点

**4. 网络结构优化**
- **重要性评估**：梯度大小反映参数的重要程度
- **剪枝依据**：梯度小的连接可能可以被删除
- **网络设计**：指导激活函数和网络结构的选择

**实际意义：**
没有梯度，神经网络就无法学习 - 梯度是神经网络从数据中学习模式的核心机制。
---

## 前馈神经网络基础

### 1. 网络结构定义

#### 1.1 基本架构
前馈神经网络（Feedforward Neural Network, FNN）是最基础的人工神经网络：

```
输入层 → 隐藏层1 → 隐藏层2 → ... → 隐藏层N → 输出层
```

**关键特征：**
- **前馈性**：信息只能从前向后传播，无反馈连接
- **层次性**：网络分为多个层次，每层包含多个神经元
- **全连接**：层与层之间的神经元完全连接
- **非线性**：通过激活函数引入非线性变换

#### 1.2 数学表示
对于L层的前馈神经网络：

**第l层的计算：**
$$z^{(l)} = W^{(l)} a^{(l-1)} + b^{(l)}$$
$$a^{(l)} = f^{(l)}(z^{(l)})$$

其中：
- $a^{(l)}$ 是第l层的激活值（输出）
- $W^{(l)}$ 是第l层的权重矩阵
- $b^{(l)}$ 是第l层的偏置向量
- $f^{(l)}$ 是第l层的激活函数
- $z^{(l)}$ 是第l层的线性组合（激活前）

**完整的前向传播：**
$$a^{(0)} = x \text{ (输入)}$$
$$a^{(L)} = f^{(L)}(W^{(L)} f^{(L-1)}(W^{(L-1)} \cdots f^{(1)}(W^{(1)} x + b^{(1)}) \cdots + b^{(L-1)}) + b^{(L)})$$

### 2. 参数计算和复杂度分析

#### 2.1 参数量计算

**单层参数量：**
设第l-1层有$n_{l-1}$个神经元，第l层有$n_l$个神经元：
- **权重参数**：$n_{l-1} \times n_l$
- **偏置参数**：$n_l$
- **总计**：$n_{l-1} \times n_l + n_l = n_l(n_{l-1} + 1)$

**全网络参数量：**
$$\text{总参数量} = \sum_{l=1}^{L} n_l(n_{l-1} + 1)$$

#### 2.2 计算复杂度

**前向传播复杂度：**
- **时间复杂度**：$O(\sum_{l=1}^{L} n_{l-1} \times n_l)$
- **空间复杂度**：$O(\max_l n_l)$（存储激活值）

**反向传播复杂度：**
- **时间复杂度**：$O(\sum_{l=1}^{L} n_{l-1} \times n_l)$（与前向传播相同）
- **空间复杂度**：$O(\sum_{l=1}^{L} n_l(n_{l-1} + 1))$（存储梯度）

#### 2.3 参数量示例

**三层网络示例：**
- 输入层：784维（28×28图像）
- 隐藏层：512维
- 输出层：10维（10分类）

参数计算：
- 第一层：784 × 512 + 512 = 401,920
- 第二层：512 × 10 + 10 = 5,130
- 总计：407,050个参数

### 3. 万能逼近定理

#### 3.1 定理陈述

**万能逼近定理（Universal Approximation Theorem）：**
设$f$是$[0,1]^n$上的连续函数，$\sigma$是非常数、有界、单调递增的连续函数。那么对于任意$\epsilon > 0$，存在整数$N$，实数$v_i, b_i \in \mathbb{R}$和向量$w_i \in \mathbb{R}^n$，使得：

$$F(x) = \sum_{i=1}^{N} v_i \sigma(w_i^T x + b_i)$$

满足：
$$\sup_{x \in [0,1]^n} |f(x) - F(x)| < \epsilon$$

#### 3.2 定理意义

**理论意义：**
- **存在性保证**：证明了神经网络理论上的表达能力
- **普适性**：适用于任意连续函数
- **基础地位**：为神经网络发展提供理论支撑

**实际限制：**
- **非构造性**：不告诉我们如何找到这样的网络
- **参数爆炸**：可能需要指数级的神经元数量
- **训练困难**：不保证能通过梯度下降找到最优解
- **泛化问题**：不考虑测试集上的表现

#### 3.3 深度的重要性

虽然单隐藏层网络理论上能逼近任意函数，但**深度网络有明显优势**：

**表达效率：**
- 深网络用更少参数表达相同复杂度函数
- 某些函数的表示深度呈指数级减少参数需求

**层次表示：**
- 每层学习不同抽象层次的特征
- 底层学习简单特征，高层学习复杂概念

**优化友好：**
- 现代优化技术更适合深度网络
- 残差连接等技术解决了深度训练问题

### 4. 网络深度vs宽度权衡

#### 4.1 深度的优势

**表达能力：**
- **层次特征学习**：每层提取不同抽象层次的特征
- **组合复杂性**：深层能表示浅层需指数个神经元的函数
- **参数效率**：同样表达能力下参数更少

**优化特性：**
- **梯度流动**：残差连接等技术改善深度网络训练
- **正则化效果**：深度本身具有隐式正则化作用

#### 4.2 宽度的优势

**训练稳定性：**
- **梯度稳定**：避免深度网络的梯度消失问题
- **并行友好**：同层神经元可以并行计算
- **收敛保证**：宽网络有更好的理论收敛性质

**实现简单：**
- **架构简单**：避免复杂的连接设计
- **调参容易**：超参数相对容易调节

#### 4.3 权衡策略

**任务导向：**
- **简单任务**：浅而宽的网络可能足够
- **复杂任务**：深度网络通常表现更好
- **层次特征**：需要学习层次特征时优选深度

**资源约束：**
- **计算资源**：深度网络通常需要更多计算
- **内存限制**：考虑激活值存储需求
- **训练时间**：深度网络训练通常更慢

**实践原则：**
1. **先深度后宽度**：通常优先增加深度
2. **适度深度**：避免过深导致的优化困难
3. **实验验证**：在具体任务上验证效果

### 5. 前馈网络的变种

#### 5.1 多层感知机（MLP）

**标准MLP结构：**
- 多个全连接隐藏层
- 每层使用相同激活函数
- 输出层根据任务选择激活函数

#### 5.2 深度全连接网络

**现代改进：**
- **BatchNorm**：批归一化改善训练
- **Dropout**：随机失活防止过拟合
- **残差连接**：缓解梯度消失问题

#### 5.3 专门化结构

**任务特化：**
- **分类网络**：Softmax输出层
- **回归网络**：线性输出层
- **自编码器**：对称的编码-解码结构

## 前向传播和反向传播

### 1. 前向传播过程

#### 1.1 算法步骤

```python
def forward_propagation(X, parameters):
    """
    前向传播算法
    
    Args:
        X: 输入数据 (n_features, m_samples)
        parameters: 网络参数字典
    
    Returns:
        cache: 中间计算结果
        A_final: 最终输出
    """
    cache = {}
    A = X
    L = len(parameters) // 2  # 层数
    
    # 遍历所有隐藏层
    for l in range(1, L):
        A_prev = A
        W = parameters[f'W{l}']
        b = parameters[f'b{l}']
        
        # 线性计算
        Z = np.dot(W, A_prev) + b
        # 非线性激活
        A = activation_function(Z)
        
        # 保存中间结果用于反向传播
        cache[f'A{l-1}'] = A_prev
        cache[f'Z{l}'] = Z
        cache[f'A{l}'] = A
    
    # 输出层
    W_final = parameters[f'W{L}']
    b_final = parameters[f'b{L}']
    Z_final = np.dot(W_final, A) + b_final
    A_final = output_activation(Z_final)
    
    cache[f'A{L-1}'] = A
    cache[f'Z{L}'] = Z_final
    cache[f'A{L}'] = A_final
    
    return cache, A_final
```

#### 1.2 计算图表示

前向传播可以表示为计算图：
```
输入 → 线性变换 → 激活函数 → 线性变换 → 激活函数 → ... → 输出
```

### 2. 反向传播算法

#### 2.1 梯度计算

基于链式法则的梯度计算：

**输出层梯度：**
$$\frac{\partial L}{\partial z^{(L)}} = \frac{\partial L}{\partial a^{(L)}} \cdot \frac{\partial a^{(L)}}{\partial z^{(L)}}$$

**隐藏层梯度：**
$$\frac{\partial L}{\partial z^{(l)}} = \frac{\partial L}{\partial z^{(l+1)}} \cdot \frac{\partial z^{(l+1)}}{\partial a^{(l)}} \cdot \frac{\partial a^{(l)}}{\partial z^{(l)}}$$

**参数梯度：**
$$\frac{\partial L}{\partial W^{(l)}} = \frac{\partial L}{\partial z^{(l)}} \cdot \frac{\partial z^{(l)}}{\partial W^{(l)}} = \frac{\partial L}{\partial z^{(l)}} \cdot (a^{(l-1)})^T$$

$$\frac{\partial L}{\partial b^{(l)}} = \frac{\partial L}{\partial z^{(l)}}$$

#### 2.2 算法实现

```python
def backward_propagation(cache, parameters, Y):
    """
    反向传播算法
    
    Args:
        cache: 前向传播的中间结果
        parameters: 网络参数
        Y: 真实标签
    
    Returns:
        gradients: 参数梯度
    """
    gradients = {}
    L = len(parameters) // 2
    m = Y.shape[1]  # 样本数量
    
    # 输出层梯度
    A_final = cache[f'A{L}']
    dZ = A_final - Y  # 假设使用softmax + 交叉熵
    
    # 从后向前计算梯度
    for l in range(L, 0, -1):
        A_prev = cache[f'A{l-1}']
        
        # 参数梯度
        gradients[f'dW{l}'] = (1/m) * np.dot(dZ, A_prev.T)
        gradients[f'db{l}'] = (1/m) * np.sum(dZ, axis=1, keepdims=True)
        
        # 传播到前一层
        if l > 1:
            W = parameters[f'W{l}']
            dA_prev = np.dot(W.T, dZ)
            Z_prev = cache[f'Z{l-1}']
            dZ = dA_prev * activation_derivative(Z_prev)
    
    return gradients
```

## 面试常见问题与回答

### Q1: 前馈神经网络与其他网络类型的区别？

**回答要点：**
- **vs循环神经网络**：前馈无循环连接，RNN有时间上的循环
- **vs卷积神经网络**：前馈是全连接，CNN有局部连接和权重共享
- **vs图神经网络**：前馈有固定拓扑，GNN处理图结构数据
- **基础地位**：前馈网络是其他复杂网络的基本组件

### Q2: 如何选择网络的层数和每层的神经元数量？

**回答要点：**
- **任务复杂度**：复杂任务需要更深更宽的网络
- **数据量**：数据量大可以支持更复杂的网络
- **计算资源**：考虑训练和推理的计算成本
- **实验验证**：通过交叉验证等方法确定最优结构
- **经验法则**：先增加深度，再考虑宽度

### Q3: 前馈网络的主要局限性是什么？

**回答要点：**
- **参数量大**：全连接导致参数数量随层数平方增长
- **梯度消失**：深度网络训练困难（已部分解决）
- **缺乏先验**：没有利用数据的结构信息（如图像的局部性）
- **过拟合风险**：大量参数容易过拟合
- **计算开销**：全连接计算量大

### Q4: 如何初始化前馈网络的权重？

**回答要点：**
- **避免对称性**：随机初始化打破对称性
- **控制方差**：使用Xavier、He等初始化方法
- **考虑激活函数**：不同激活函数需要不同初始化策略
- **批归一化**：使用BN时初始化要求相对宽松

### Q5: 前馈网络在现代深度学习中的地位如何？

**回答要点：**
- **基础组件**：是Transformer、ResNet等的基本构建块
- **特定应用**：在表格数据、简单分类任务中仍然有效
- **理论基础**：为理解其他网络提供基础
- **教学价值**：是学习深度学习的重要起点

## 代码实现示例

### 1. 从零实现前馈神经网络

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_classification, make_circles
from sklearn.model_selection import train_test_split
import matplotlib
matplotlib.rcParams['font.sans-serif'] = ['SimHei']
matplotlib.rcParams['axes.unicode_minus'] = False

class FeedforwardNeuralNetwork:
    def __init__(self, layer_sizes, learning_rate=0.01, activation='relu'):
        """
        初始化前馈神经网络
        
        Args:
            layer_sizes: 每层神经元数量列表，如[784, 128, 64, 10]
            learning_rate: 学习率
            activation: 激活函数类型
        """
        self.layer_sizes = layer_sizes
        self.learning_rate = learning_rate
        self.activation = activation
        self.num_layers = len(layer_sizes) - 1
        
        # 初始化参数
        self.parameters = self._initialize_parameters()
        
    def _initialize_parameters(self):
        """Xavier初始化"""
        parameters = {}
        
        for l in range(1, len(self.layer_sizes)):
            # Xavier初始化
            parameters[f'W{l}'] = np.random.randn(
                self.layer_sizes[l], self.layer_sizes[l-1]
            ) * np.sqrt(1 / self.layer_sizes[l-1])
            parameters[f'b{l}'] = np.zeros((self.layer_sizes[l], 1))
            
        return parameters
    
    def _activation_function(self, Z, derivative=False):
        """激活函数"""
        if self.activation == 'relu':
            if derivative:
                return (Z > 0).astype(float)
            return np.maximum(0, Z)
        elif self.activation == 'sigmoid':
            sigmoid = 1 / (1 + np.exp(-np.clip(Z, -500, 500)))
            if derivative:
                return sigmoid * (1 - sigmoid)
            return sigmoid
        elif self.activation == 'tanh':
            if derivative:
                tanh = np.tanh(Z)
                return 1 - tanh**2
            return np.tanh(Z)
    
    def _softmax(self, Z):
        """Softmax激活函数"""
        exp_Z = np.exp(Z - np.max(Z, axis=0, keepdims=True))
        return exp_Z / np.sum(exp_Z, axis=0, keepdims=True)
    
    def forward_propagation(self, X):
        """前向传播"""
        cache = {'A0': X}
        A = X
        
        # 隐藏层
        for l in range(1, self.num_layers):
            W = self.parameters[f'W{l}']
            b = self.parameters[f'b{l}']
            
            Z = np.dot(W, A) + b
            A = self._activation_function(Z)
            
            cache[f'Z{l}'] = Z
            cache[f'A{l}'] = A
        
        # 输出层
        W = self.parameters[f'W{self.num_layers}']
        b = self.parameters[f'b{self.num_layers}']
        Z = np.dot(W, A) + b
        A = self._softmax(Z)  # 多分类使用softmax
        
        cache[f'Z{self.num_layers}'] = Z
        cache[f'A{self.num_layers}'] = A
        
        return A, cache
    
    def compute_cost(self, Y_pred, Y_true):
        """计算交叉熵损失"""
        m = Y_true.shape[1]
        cost = -np.sum(Y_true * np.log(Y_pred + 1e-8)) / m
        return cost
    
    def backward_propagation(self, cache, Y_true):
        """反向传播"""
        gradients = {}
        m = Y_true.shape[1]
        
        # 输出层梯度
        A_final = cache[f'A{self.num_layers}']
        dZ = A_final - Y_true
        
        # 从后向前传播
        for l in range(self.num_layers, 0, -1):
            A_prev = cache[f'A{l-1}']
            
            # 参数梯度
            gradients[f'dW{l}'] = (1/m) * np.dot(dZ, A_prev.T)
            gradients[f'db{l}'] = (1/m) * np.sum(dZ, axis=1, keepdims=True)
            
            # 传播到前一层
            if l > 1:
                W = self.parameters[f'W{l}']
                dA_prev = np.dot(W.T, dZ)
                Z_prev = cache[f'Z{l-1}']
                dZ = dA_prev * self._activation_function(Z_prev, derivative=True)
        
        return gradients
    
    def update_parameters(self, gradients):
        """更新参数"""
        for l in range(1, self.num_layers + 1):
            self.parameters[f'W{l}'] -= self.learning_rate * gradients[f'dW{l}']
            self.parameters[f'b{l}'] -= self.learning_rate * gradients[f'db{l}']
    
    def train(self, X, Y, epochs, print_cost=True):
        """训练网络"""
        costs = []
        
        for epoch in range(epochs):
            # 前向传播
            Y_pred, cache = self.forward_propagation(X)
            
            # 计算损失
            cost = self.compute_cost(Y_pred, Y)
            costs.append(cost)
            
            # 反向传播
            gradients = self.backward_propagation(cache, Y)
            
            # 更新参数
            self.update_parameters(gradients)
            
            # 打印损失
            if print_cost and epoch % 100 == 0:
                print(f"Epoch {epoch}, Cost: {cost:.6f}")
        
        return costs
    
    def predict(self, X):
        """预测"""
        Y_pred, _ = self.forward_propagation(X)
        predictions = np.argmax(Y_pred, axis=0)
        return predictions
    
    def get_accuracy(self, X, Y):
        """计算准确率"""
        predictions = self.predict(X)
        Y_true = np.argmax(Y, axis=0)
        accuracy = np.mean(predictions == Y_true)
        return accuracy

# 工具函数
def one_hot_encode(y, num_classes):
    """独热编码"""
    encoded = np.zeros((num_classes, len(y)))
    for i, label in enumerate(y):
        encoded[label, i] = 1
    return encoded

def plot_decision_boundary(model, X, y, title="决策边界"):
    """绘制决策边界"""
    plt.figure(figsize=(10, 8))
    
    # 创建网格
    h = 0.01
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                         np.arange(y_min, y_max, h))
    
    # 预测网格点
    mesh_points = np.c_[xx.ravel(), yy.ravel()].T
    Z = model.predict(mesh_points)
    Z = Z.reshape(xx.shape)
    
    # 绘制决策边界
    plt.contourf(xx, yy, Z, alpha=0.8, cmap=plt.cm.RdYlBu)
    
    # 绘制数据点
    scatter = plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.RdYlBu, edgecolors='black')
    plt.colorbar(scatter)
    plt.title(title)
    plt.xlabel('特征1')
    plt.ylabel('特征2')
    plt.show()

# 示例1：线性可分数据
def example_linear_data():
    print("=== 示例1：线性可分数据 ===")
    
    # 生成数据
    X, y = make_classification(n_samples=300, n_features=2, n_redundant=0, 
                              n_informative=2, n_clusters_per_class=1, random_state=42)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    
    # 预处理
    X_train = X_train.T
    X_test = X_test.T
    y_train_onehot = one_hot_encode(y_train, 2)
    y_test_onehot = one_hot_encode(y_test, 2)
    
    # 创建网络
    model = FeedforwardNeuralNetwork([2, 4, 2], learning_rate=0.1, activation='relu')
    
    # 训练
    costs = model.train(X_train, y_train_onehot, epochs=1000)
    
    # 评估
    train_acc = model.get_accuracy(X_train, y_train_onehot)
    test_acc = model.get_accuracy(X_test, y_test_onehot)
    
    print(f"训练准确率: {train_acc:.4f}")
    print(f"测试准确率: {test_acc:.4f}")
    
    # 可视化
    plot_decision_boundary(model, X.T, y, "线性可分数据 - 决策边界")
    
    # 损失曲线
    plt.figure(figsize=(8, 6))
    plt.plot(costs)
    plt.title('训练损失曲线')
    plt.xlabel('Epoch')
    plt.ylabel('Cost')
    plt.grid(True)
    plt.show()

# 示例2：非线性数据
def example_nonlinear_data():
    print("\n=== 示例2：非线性数据（同心圆）===")
    
    # 生成数据
    X, y = make_circles(n_samples=300, noise=0.1, factor=0.3, random_state=42)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    
    # 预处理
    X_train = X_train.T
    X_test = X_test.T
    y_train_onehot = one_hot_encode(y_train, 2)
    y_test_onehot = one_hot_encode(y_test, 2)
    
    # 创建网络（更深的网络处理非线性数据）
    model = FeedforwardNeuralNetwork([2, 16, 8, 2], learning_rate=0.1, activation='relu')
    
    # 训练
    costs = model.train(X_train, y_train_onehot, epochs=2000)
    
    # 评估
    train_acc = model.get_accuracy(X_train, y_train_onehot)
    test_acc = model.get_accuracy(X_test, y_test_onehot)
    
    print(f"训练准确率: {train_acc:.4f}")
    print(f"测试准确率: {test_acc:.4f}")
    
    # 可视化
    plot_decision_boundary(model, X.T, y, "非线性数据（同心圆）- 决策边界")

# 示例3：网络深度影响
def compare_network_depths():
    print("\n=== 示例3：网络深度对比 ===")
    
    # 生成复杂的非线性数据
    X, y = make_circles(n_samples=500, noise=0.1, factor=0.2, random_state=42)
    X = X.T
    y_onehot = one_hot_encode(y, 2)
    
    # 不同深度的网络
    architectures = {
        '浅层网络 [2-8-2]': [2, 8, 2],
        '中等网络 [2-16-8-2]': [2, 16, 8, 2],
        '深层网络 [2-32-16-8-2]': [2, 32, 16, 8, 2]
    }
    
    results = {}
    
    for name, architecture in architectures.items():
        print(f"\n训练 {name}...")
        
        model = FeedforwardNeuralNetwork(architecture, learning_rate=0.05, activation='relu')
        costs = model.train(X, y_onehot, epochs=1500, print_cost=False)
        accuracy = model.get_accuracy(X, y_onehot)
        
        results[name] = {
            'model': model,
            'costs': costs,
            'accuracy': accuracy,
            'params': sum(architecture[i] * architecture[i-1] + architecture[i] 
                         for i in range(1, len(architecture)))
        }
        
        print(f"准确率: {accuracy:.4f}, 参数量: {results[name]['params']}")
    
    # 可视化对比
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    
    # 损失曲线对比
    axes[0, 0].set_title('训练损失对比')
    for name, result in results.items():
        axes[0, 0].plot(result['costs'], label=name, alpha=0.8)
    axes[0, 0].set_xlabel('Epoch')
    axes[0, 0].set_ylabel('Cost')
    axes[0, 0].legend()
    axes[0, 0].grid(True)
    
    # 准确率对比
    names = list(results.keys())
    accuracies = [results[name]['accuracy'] for name in names]
    params = [results[name]['params'] for name in names]
    
    bars = axes[0, 1].bar(range(len(names)), accuracies, alpha=0.7)
    axes[0, 1].set_title('准确率对比')
    axes[0, 1].set_ylabel('Accuracy')
    axes[0, 1].set_xticks(range(len(names)))
    axes[0, 1].set_xticklabels([name.split('[')[0].strip() for name in names], rotation=45)
    
    for bar, acc in zip(bars, accuracies):
        height = bar.get_height()
        axes[0, 1].text(bar.get_x() + bar.get_width()/2., height + 0.01,
                       f'{acc:.3f}', ha='center', va='bottom')
    
    # 参数量对比
    bars = axes[1, 0].bar(range(len(names)), params, alpha=0.7, color='orange')
    axes[1, 0].set_title('参数量对比')
    axes[1, 0].set_ylabel('参数数量')
    axes[1, 0].set_xticks(range(len(names)))
    axes[1, 0].set_xticklabels([name.split('[')[0].strip() for name in names], rotation=45)
    
    for bar, param in zip(bars, params):
        height = bar.get_height()
        axes[1, 0].text(bar.get_x() + bar.get_width()/2., height + 10,
                       f'{param}', ha='center', va='bottom')
    
    # 最佳模型的决策边界
    best_model_name = max(results.keys(), key=lambda x: results[x]['accuracy'])
    best_model = results[best_model_name]['model']
    
    # 创建网格用于决策边界
    h = 0.01
    x_min, x_max = X[0, :].min() - 1, X[0, :].max() + 1
    y_min, y_max = X[1, :].min() - 1, X[1, :].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                         np.arange(y_min, y_max, h))
    
    mesh_points = np.c_[xx.ravel(), yy.ravel()].T
    Z = best_model.predict(mesh_points)
    Z = Z.reshape(xx.shape)
    
    axes[1, 1].contourf(xx, yy, Z, alpha=0.8, cmap=plt.cm.RdYlBu)
    scatter = axes[1, 1].scatter(X[0, :], X[1, :], c=y, cmap=plt.cm.RdYlBu, edgecolors='black')
    axes[1, 1].set_title(f'最佳模型决策边界\n{best_model_name}')
    axes[1, 1].set_xlabel('特征1')
    axes[1, 1].set_ylabel('特征2')
    
    plt.tight_layout()
    plt.show()

# 主函数
if __name__ == "__main__":
    example_linear_data()
    example_nonlinear_data()
    compare_network_depths()
```

### 2. 使用PyTorch实现

```python
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import DataLoader, TensorDataset

class FeedforwardNet(nn.Module):
    def __init__(self, input_size, hidden_sizes, output_size, activation='relu', dropout=0.0):
        super(FeedforwardNet, self).__init__()
        
        layers = []
        prev_size = input_size
        
        # 隐藏层
        for hidden_size in hidden_sizes:
            layers.append(nn.Linear(prev_size, hidden_size))
            
            if activation == 'relu':
                layers.append(nn.ReLU())
            elif activation == 'tanh':
                layers.append(nn.Tanh())
            elif activation == 'sigmoid':
                layers.append(nn.Sigmoid())
            
            if dropout > 0:
                layers.append(nn.Dropout(dropout))
            
            prev_size = hidden_size
        
        # 输出层
        layers.append(nn.Linear(prev_size, output_size))
        
        self.network = nn.Sequential(*layers)
    
    def forward(self, x):
        return self.network(x)

def train_pytorch_model():
    """使用PyTorch训练前馈网络"""
    print("=== PyTorch实现示例 ===")
    
    # 生成数据
    X, y = make_classification(n_samples=1000, n_features=20, n_classes=3, 
                              n_informative=15, n_redundant=5, random_state=42)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    
    # 转换为tensor
    X_train = torch.FloatTensor(X_train)
    X_test = torch.FloatTensor(X_test)
    y_train = torch.LongTensor(y_train)
    y_test = torch.LongTensor(y_test)
    
    # 创建数据加载器
    train_dataset = TensorDataset(X_train, y_train)
    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
    
    # 创建模型
    model = FeedforwardNet(input_size=20, hidden_sizes=[64, 32], output_size=3, 
                          activation='relu', dropout=0.2)
    
    # 损失函数和优化器
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    
    # 训练
    model.train()
    for epoch in range(100):
        total_loss = 0
        for batch_X, batch_y in train_loader:
            optimizer.zero_grad()
            outputs = model(batch_X)
            loss = criterion(outputs, batch_y)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        
        if (epoch + 1) % 20 == 0:
            print(f'Epoch [{epoch+1}/100], Loss: {total_loss/len(train_loader):.4f}')
    
    # 评估
    model.eval()
    with torch.no_grad():
        train_outputs = model(X_train)
        _, train_predicted = torch.max(train_outputs.data, 1)
        train_accuracy = (train_predicted == y_train).sum().item() / len(y_train)
        
        test_outputs = model(X_test)
        _, test_predicted = torch.max(test_outputs.data, 1)
        test_accuracy = (test_predicted == y_test).sum().item() / len(y_test)
    
    print(f'训练准确率: {train_accuracy:.4f}')
    print(f'测试准确率: {test_accuracy:.4f}')
    
    # 计算参数量
    total_params = sum(p.numel() for p in model.parameters())
    print(f'总参数量: {total_params}')

if __name__ == "__main__":
    train_pytorch_model()
```

## 关键概念总结

### 理论基础
- **前馈性质**：信息单向流动，无循环连接
- **万能逼近定理**：理论上的强大表达能力
- **层次结构**：多层次的特征学习
- **参数效率**：深度网络的表达优势

### 技术要点
- **参数计算**：权重矩阵和偏置向量的规模
- **前向传播**：从输入到输出的计算过程
- **反向传播**：基于梯度的参数更新
- **网络设计**：深度与宽度的权衡

### 实践指导
- **结构选择**：根据任务复杂度确定网络规模
- **初始化策略**：合理的权重初始化
- **训练技巧**：学习率、正则化、批归一化等
- **性能优化**：考虑计算效率和内存使用

---

## 相关笔记
<!-- 自动生成 -->

- [反向传播算法](notes/深度学习/反向传播算法.md) - 相似度: 36% | 标签: 深度学习, 深度学习/反向传播算法.md
- [多层感知机解决非线性问题的能力](notes/深度学习/多层感知机解决非线性问题的能力.md) - 相似度: 31% | 标签: 深度学习, 深度学习/多层感知机解决非线性问题的能力.md
- [残差连接（ResNet）的原理和效果](notes/深度学习/残差连接（ResNet）的原理和效果.md) - 相似度: 31% | 标签: 深度学习, 深度学习/残差连接（ResNet）的原理和效果.md

