---
created: '2025-10-19'
last_reviewed: null
next_review: '2025-10-19'
review_count: 0
difficulty: medium
mastery_level: 0.0
tags:
- æ·±åº¦å­¦ä¹ 
- æ·±åº¦å­¦ä¹ /æŸå¤±å‡½æ•°è®¾è®¡.md
related_outlines: []
---
# æŸå¤±å‡½æ•°è®¾è®¡

## é¢è¯•é‡ç‚¹é—®é¢˜

### 1. ä¸ºä»€ä¹ˆåˆ†ç±»ç”¨äº¤å‰ç†µè€Œä¸æ˜¯MSEï¼Ÿ
åˆ†ç±»ä»»åŠ¡ä¸­æ›´å¸¸ç”¨äº¤å‰ç†µè€Œä¸æ˜¯å‡æ–¹è¯¯å·®ï¼ŒåŸå› åœ¨äºæ¢¯åº¦ä¼ æ’­çš„å·®å¼‚ï¼š

å¦‚æœç”¨ MSE + Softmaxï¼Œåå‘ä¼ æ’­æ—¶æ¢¯åº¦é‡ŒåŒ…å«äº† softmax çš„å¯¼æ•°ã€‚å½“ softmax è¾“å‡ºæ¥è¿‘ 0 æˆ– 1 æ—¶ï¼Œå¯¼æ•°è¶‹è¿‘äº 0ï¼Œå®¹æ˜“é€ æˆæ¢¯åº¦æ¶ˆå¤±ï¼Œè®­ç»ƒéå¸¸æ…¢ã€‚

å¦‚æœç”¨ Cross-Entropy + Softmaxï¼Œæ¢¯åº¦å¯ä»¥ç®€åŒ–æˆ p^âˆ’yï¼Œç›´æ¥æ˜¯é¢„æµ‹æ¦‚ç‡å’ŒçœŸå®æ ‡ç­¾çš„å·®å€¼ï¼Œæ¢¯åº¦ä¼ æ’­ç¨³å®šé«˜æ•ˆï¼Œä¸ä¼šå‡ºç°ä¸¥é‡çš„æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ã€‚

æ‰€ä»¥åœ¨åˆ†ç±»é‡Œæˆ‘ä»¬ä¸€èˆ¬ä¸ç”¨ MSEï¼Œè€Œæ˜¯ç”¨äº¤å‰ç†µã€‚

**æ•°å­¦åŸå› **ï¼š
- **æ¢¯åº¦ç‰¹æ€§**ï¼šäº¤å‰ç†µæ¢¯åº¦ä¸é¢„æµ‹è¯¯å·®æˆæ­£æ¯”ï¼ŒMSEæ¢¯åº¦ä¸æ¿€æ´»å‡½æ•°å¯¼æ•°ç›¸å…³
- **é¥±å’Œé—®é¢˜**ï¼šMSEåœ¨sigmoidé¥±å’ŒåŒºæ¢¯åº¦å¾ˆå°ï¼Œå­¦ä¹ ç¼“æ…¢

**å…·ä½“åˆ†æ**ï¼š
MSE + Sigmoidï¼š
$$\frac{\partial L}{\partial z} = (a-y) \cdot \sigma'(z)$$
åœ¨é¥±å’ŒåŒº$\sigma'(z) \approx 0$ï¼Œæ¢¯åº¦æ¶ˆå¤±ã€‚

äº¤å‰ç†µ + Sigmoidï¼š
$$\frac{\partial L}{\partial z} = a - y$$
æ¢¯åº¦ä¸è¯¯å·®ç›´æ¥ç›¸å…³ï¼Œä¸å—æ¿€æ´»å‡½æ•°å½±å“ã€‚

### 2. å¦‚ä½•é€‰æ‹©åˆé€‚çš„æŸå¤±å‡½æ•°ï¼Ÿ

**å†³ç­–æ ‘**ï¼š
```
ä»»åŠ¡ç±»å‹ï¼Ÿ
â”œâ”€â”€ å›å½’
â”‚   â”œâ”€â”€ æœ‰å¼‚å¸¸å€¼ï¼Ÿâ†’ MAEæˆ–Huber
â”‚   â””â”€â”€ æ— å¼‚å¸¸å€¼ï¼Ÿâ†’ MSE
â”œâ”€â”€ åˆ†ç±»
â”‚   â”œâ”€â”€ äºŒåˆ†ç±»ï¼Ÿâ†’ Binary Cross-Entropy
â”‚   â”œâ”€â”€ å¤šåˆ†ç±»ï¼Ÿâ†’ Categorical Cross-Entropy
â”‚   â””â”€â”€ ç±»åˆ«ä¸å¹³è¡¡ï¼Ÿâ†’ Focal Lossæˆ–åŠ æƒäº¤å‰ç†µ
â””â”€â”€ åºåˆ—
    â”œâ”€â”€ æœ‰å¯¹é½ï¼Ÿâ†’ æ ‡å‡†äº¤å‰ç†µ
    â””â”€â”€ æ— å¯¹é½ï¼Ÿâ†’ CTC Loss
```

### 3. æŸå¤±å‡½æ•°çš„æ•°å€¼ç¨³å®šæ€§

**å¸¸è§é—®é¢˜**ï¼š
- **log(0)**ï¼šäº¤å‰ç†µä¸­é¢„æµ‹æ¦‚ç‡ä¸º0
- **expæº¢å‡º**ï¼šsoftmaxä¸­æŒ‡æ•°è¿ç®—
- **æ¢¯åº¦çˆ†ç‚¸/æ¶ˆå¤±**ï¼šæç«¯é¢„æµ‹å€¼

**è§£å†³æ–¹æ¡ˆ**ï¼š
```python
def stable_softmax(logits):
    # å‡å»æœ€å¤§å€¼é˜²æ­¢æº¢å‡º
    shifted = logits - np.max(logits, axis=-1, keepdims=True)
    exp_shifted = np.exp(shifted)
    return exp_shifted / np.sum(exp_shifted, axis=-1, keepdims=True)

def stable_cross_entropy(y_true, y_pred, epsilon=1e-15):
    # è£å‰ªé˜²æ­¢log(0)
    y_pred = np.clip(y_pred, epsilon, 1-epsilon)
    return -np.sum(y_true * np.log(y_pred))
```

### 4. æ­£åˆ™åŒ–å‚æ•°å¦‚ä½•é€‰æ‹©ï¼Ÿ

**æ–¹æ³•**ï¼š
1. **äº¤å‰éªŒè¯**ï¼šç½‘æ ¼æœç´¢æˆ–éšæœºæœç´¢
2. **éªŒè¯æ›²çº¿**ï¼šè§‚å¯ŸéªŒè¯è¯¯å·®éš$\lambda$å˜åŒ–
3. **æ—©åœæ³•**ï¼šåŸºäºéªŒè¯é›†æ€§èƒ½
4. **è´å¶æ–¯ä¼˜åŒ–**ï¼šé«˜æ•ˆè¶…å‚æ•°æœç´¢

**å®è·µæŠ€å·§**ï¼š
- ä»å¤§èŒƒå›´å¼€å§‹ï¼š$[10^{-5}, 10^{-1}]$
- è§‚å¯Ÿè®­ç»ƒ/éªŒè¯æ›²çº¿çš„åˆ†ç¦»ç¨‹åº¦
- L1æ­£åˆ™åŒ–é€šå¸¸éœ€è¦è¾ƒå°çš„$\lambda$

### 5. è‡ªå®šä¹‰æŸå¤±å‡½æ•°çš„è®¾è®¡åŸåˆ™

**è®¾è®¡æ­¥éª¤**ï¼š
1. **æ˜ç¡®ç›®æ ‡**ï¼šæŸå¤±å‡½æ•°åº”è¯¥åæ˜ çœŸå®è¯„ä»·æŒ‡æ ‡
2. **å¯å¾®æ€§**ï¼šç¡®ä¿æ¢¯åº¦å­˜åœ¨ä¸”è®¡ç®—é«˜æ•ˆ
3. **æ•°å€¼ç¨³å®šæ€§**ï¼šé¿å…æ•°å€¼è®¡ç®—é—®é¢˜
4. **å‡¸æ€§åˆ†æ**ï¼šç†è§£ä¼˜åŒ–éš¾åº¦

**å®ä¾‹ï¼šIoU Loss**
```python
def iou_loss(y_true, y_pred, smooth=1e-6):
    # è®¡ç®—äº¤å¹¶æ¯”æŸå¤±
    intersection = np.sum(y_true * y_pred)
    union = np.sum(y_true) + np.sum(y_pred) - intersection
    iou = (intersection + smooth) / (union + smooth)
    return 1 - iou
```

## æ ¸å¿ƒæ¦‚å¿µ

### ä»€ä¹ˆæ˜¯æŸå¤±å‡½æ•°
æŸå¤±å‡½æ•°ï¼ˆLoss Functionï¼‰æ˜¯è¡¡é‡æ¨¡å‹é¢„æµ‹å€¼ä¸çœŸå®å€¼ä¹‹é—´å·®å¼‚çš„å‡½æ•°ï¼Œæ˜¯æ·±åº¦å­¦ä¹ ä¸­ä¼˜åŒ–ç›®æ ‡çš„æ•°å­¦è¡¨è¾¾ã€‚

**å…³é”®ä½œç”¨**ï¼š
- æä¾›ä¼˜åŒ–æ–¹å‘ï¼šæ¢¯åº¦ä¸‹é™çš„ç›®æ ‡
- é‡åŒ–æ¨¡å‹æ€§èƒ½ï¼šè®­ç»ƒè¿‡ç¨‹çš„æŒ‡å¯¼ä¿¡å·
- ä»»åŠ¡é€‚é…ï¼šä¸åŒä»»åŠ¡éœ€è¦ä¸åŒçš„æŸå¤±å‡½æ•°

**è®¾è®¡åŸåˆ™**ï¼š
- å¯å¾®æ€§ï¼šæ”¯æŒæ¢¯åº¦è®¡ç®—
- åˆç†æ€§ï¼šç¬¦åˆä»»åŠ¡è¯­ä¹‰
- æ•°å€¼ç¨³å®šæ€§ï¼šé¿å…æ•°å€¼è®¡ç®—é—®é¢˜

## ä¸åŒä»»åŠ¡ç±»å‹çš„æŸå¤±å‡½æ•°é€‰æ‹©

### ä»»åŠ¡ç±»å‹å®šä¹‰

#### å›å½’ä»»åŠ¡ï¼ˆRegressionï¼‰
**å®šä¹‰**ï¼šé¢„æµ‹è¿ç»­æ•°å€¼çš„ä»»åŠ¡ï¼Œè¾“å‡ºç©ºé—´ä¸ºå®æ•°åŸŸæˆ–å®æ•°åŸŸçš„å­é›†ã€‚

**ç‰¹å¾**ï¼š
- è¾“å‡ºå€¼è¿ç»­ä¸”æœ‰åº
- é¢„æµ‹è¯¯å·®å¯ä»¥é‡åŒ–è·ç¦»
- ç›®æ ‡æ˜¯æœ€å°åŒ–é¢„æµ‹å€¼ä¸çœŸå®å€¼çš„åå·®

**å…¸å‹åº”ç”¨**ï¼šæˆ¿ä»·é¢„æµ‹ã€è‚¡ç¥¨ä»·æ ¼é¢„æµ‹ã€æ¸©åº¦é¢„æµ‹ã€è¯„åˆ†é¢„æµ‹

#### åˆ†ç±»ä»»åŠ¡ï¼ˆClassificationï¼‰
**å®šä¹‰**ï¼šå°†è¾“å…¥æ ·æœ¬åˆ†é…åˆ°é¢„å®šä¹‰ç±»åˆ«ä¸­çš„ä»»åŠ¡ï¼Œè¾“å‡ºä¸ºç¦»æ•£çš„ç±»åˆ«æ ‡ç­¾ã€‚

**ç‰¹å¾**ï¼š
- è¾“å‡ºå€¼ç¦»æ•£ä¸”é€šå¸¸æ— åºï¼ˆå¤šåˆ†ç±»ï¼‰æˆ–æœ‰åºï¼ˆäºŒåˆ†ç±»ï¼‰
- å…³æ³¨åˆ†ç±»è¾¹ç•Œçš„å‡†ç¡®æ€§
- ç›®æ ‡æ˜¯æœ€å¤§åŒ–åˆ†ç±»å‡†ç¡®ç‡

**åˆ†ç±»å­ç±»å‹**ï¼š
- **äºŒåˆ†ç±»**ï¼šåªæœ‰ä¸¤ä¸ªç±»åˆ«ï¼ˆå¦‚åƒåœ¾é‚®ä»¶æ£€æµ‹ï¼‰
- **å¤šåˆ†ç±»**ï¼šå¤šä¸ªäº’æ–¥ç±»åˆ«ï¼ˆå¦‚å›¾åƒåˆ†ç±»ï¼‰
- **å¤šæ ‡ç­¾åˆ†ç±»**ï¼šæ ·æœ¬å¯å±äºå¤šä¸ªç±»åˆ«ï¼ˆå¦‚æ–‡æœ¬æ ‡ç­¾ï¼‰

**å…¸å‹åº”ç”¨**ï¼šå›¾åƒè¯†åˆ«ã€æ–‡æœ¬åˆ†ç±»ã€æƒ…æ„Ÿåˆ†æã€åŒ»ç–—è¯Šæ–­

#### åºåˆ—å»ºæ¨¡ä»»åŠ¡ï¼ˆSequence Modelingï¼‰
**å®šä¹‰**ï¼šå¤„ç†åºåˆ—æ•°æ®çš„ä»»åŠ¡ï¼Œè€ƒè™‘æ•°æ®çš„æ—¶åºæˆ–ä½ç½®ä¾èµ–å…³ç³»ã€‚

**ç‰¹å¾**ï¼š
- è¾“å…¥è¾“å‡ºå…·æœ‰åºåˆ—ç»“æ„
- éœ€è¦å»ºæ¨¡å…ƒç´ é—´çš„ä¾èµ–å…³ç³»
- å¯èƒ½æ¶‰åŠå˜é•¿åºåˆ—

**åºåˆ—ä»»åŠ¡å­ç±»å‹**ï¼š
- **åºåˆ—åˆ°åºåˆ—ï¼ˆSeq2Seqï¼‰**ï¼šæœºå™¨ç¿»è¯‘ã€æ–‡æœ¬æ‘˜è¦
- **åºåˆ—æ ‡æ³¨**ï¼šè¯æ€§æ ‡æ³¨ã€å‘½åå®ä½“è¯†åˆ«
- **åºåˆ—ç”Ÿæˆ**ï¼šè¯­è¨€æ¨¡å‹ã€éŸ³ä¹ç”Ÿæˆ

#### å…¶ä»–ç‰¹æ®Šä»»åŠ¡
- **åˆ†å‰²ä»»åŠ¡**ï¼šåƒç´ çº§åˆ«çš„åˆ†ç±»ï¼ˆå›¾åƒåˆ†å‰²ï¼‰
- **æ£€æµ‹ä»»åŠ¡**ï¼šå®šä½+åˆ†ç±»çš„ç»„åˆï¼ˆç›®æ ‡æ£€æµ‹ï¼‰
- **æ’åºä»»åŠ¡**ï¼šå­¦ä¹ ç›¸å¯¹æ’åºå…³ç³»
- **èšç±»ä»»åŠ¡**ï¼šæ— ç›‘ç£çš„æ ·æœ¬åˆ†ç»„

### 1. å›å½’ä»»åŠ¡

#### å‡æ–¹è¯¯å·®ï¼ˆMean Squared Error, MSEï¼‰
$$L_{MSE} = \frac{1}{n}\sum_{i=1}^{n}(y_i - \hat{y}_i)^2$$

**ç‰¹ç‚¹**ï¼š
- å¯¹å¼‚å¸¸å€¼æ•æ„Ÿï¼ˆå¹³æ–¹æ”¾å¤§è¯¯å·®ï¼‰
- å¯å¾®ä¸”å‡¸å‡½æ•°
- å‡è®¾è¯¯å·®æœä»é«˜æ–¯åˆ†å¸ƒ

**é€‚ç”¨åœºæ™¯**ï¼š
- è¿ç»­å€¼é¢„æµ‹
- è¯¯å·®åˆ†å¸ƒè¾ƒå‡åŒ€çš„ä»»åŠ¡
- å¯¹å¤§è¯¯å·®æœ‰å¼ºæƒ©ç½šéœ€æ±‚

**ä»£ç å®ç°**ï¼š
```python
def mse_loss(y_true, y_pred):
    return np.mean((y_true - y_pred) ** 2)

def mse_gradient(y_true, y_pred):
    return 2 * (y_pred - y_true) / len(y_true)
```

#### å¹³å‡ç»å¯¹è¯¯å·®ï¼ˆMean Absolute Error, MAEï¼‰
$$L_{MAE} = \frac{1}{n}\sum_{i=1}^{n}|y_i - \hat{y}_i|$$

**ç‰¹ç‚¹**ï¼š
- å¯¹å¼‚å¸¸å€¼ä¸æ•æ„Ÿï¼ˆçº¿æ€§æƒ©ç½šï¼‰
- åœ¨0å¤„ä¸å¯å¾®
- å‡è®¾è¯¯å·®æœä»æ‹‰æ™®æ‹‰æ–¯åˆ†å¸ƒ

**é€‚ç”¨åœºæ™¯**ï¼š
- å­˜åœ¨å¼‚å¸¸å€¼çš„æ•°æ®
- å¯¹æ‰€æœ‰è¯¯å·®ç­‰æƒé‡å¤„ç†
- é²æ£’æ€§è¦æ±‚é«˜

#### HuberæŸå¤±
$$L_{Huber} = \begin{cases}
\frac{1}{2}(y - \hat{y})^2 & |y - \hat{y}| \leq \delta \\
\delta|y - \hat{y}| - \frac{1}{2}\delta^2 & |y - \hat{y}| > \delta
\end{cases}$$

**ç‰¹ç‚¹**ï¼š
- ç»“åˆMSEå’ŒMAEä¼˜ç‚¹
- å°è¯¯å·®ç”¨å¹³æ–¹æŸå¤±ï¼Œå¤§è¯¯å·®ç”¨çº¿æ€§æŸå¤±
- å¤„å¤„å¯å¾®

**ä»£ç å®ç°**ï¼š
```python
def huber_loss(y_true, y_pred, delta=1.0):
    error = y_true - y_pred
    condition = np.abs(error) <= delta
    squared_loss = 0.5 * error**2
    linear_loss = delta * np.abs(error) - 0.5 * delta**2
    return np.where(condition, squared_loss, linear_loss).mean()
```

### 2. åˆ†ç±»ä»»åŠ¡

#### äº¤å‰ç†µæŸå¤±ï¼ˆCross-Entropyï¼‰

**ä»€ä¹ˆæ˜¯äº¤å‰ç†µ**ï¼š
äº¤å‰ç†µæ˜¯ä¿¡æ¯è®ºä¸­çš„æ¦‚å¿µï¼Œç”¨äºè¡¡é‡ä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒä¹‹é—´çš„å·®å¼‚ã€‚åœ¨æœºå™¨å­¦ä¹ ä¸­ï¼Œå®ƒåº¦é‡äº†é¢„æµ‹åˆ†å¸ƒä¸çœŸå®åˆ†å¸ƒä¹‹é—´çš„"è·ç¦»"ã€‚

**ä¿¡æ¯è®ºåŸºç¡€**ï¼š
- **ä¿¡æ¯é‡**ï¼š$I(x) = -\log P(x)$ï¼Œäº‹ä»¶æ¦‚ç‡è¶Šå°ï¼Œä¿¡æ¯é‡è¶Šå¤§
- **ç†µ**ï¼š$H(P) = -\sum_{i} P(i)\log P(i)$ï¼Œè¡¡é‡åˆ†å¸ƒçš„ä¸ç¡®å®šæ€§
- **äº¤å‰ç†µ**ï¼š$H(P,Q) = -\sum_{i} P(i)\log Q(i)$ï¼Œç”¨åˆ†å¸ƒQæ¥ç¼–ç åˆ†å¸ƒPæ‰€éœ€çš„å¹³å‡ä¿¡æ¯é‡
- **KLæ•£åº¦**ï¼š$D_{KL}(P||Q) = H(P,Q) - H(P)$ï¼Œè¡¡é‡ä¸¤åˆ†å¸ƒçš„å·®å¼‚

**ç›´è§‚ç†è§£**ï¼š
å½“æˆ‘ä»¬ç”¨é¢„æµ‹åˆ†å¸ƒ$\hat{y}$æ¥"æè¿°"çœŸå®åˆ†å¸ƒ$y$æ—¶ï¼Œäº¤å‰ç†µå‘Šè¯‰æˆ‘ä»¬è¿™ç§æè¿°çš„"ä»£ä»·"ã€‚é¢„æµ‹è¶Šå‡†ç¡®ï¼Œä»£ä»·è¶Šå°ã€‚

**äºŒåˆ†ç±»äº¤å‰ç†µ**ï¼š
$$L_{BCE} = -\frac{1}{n}\sum_{i=1}^{n}[y_i\log(\hat{y}_i) + (1-y_i)\log(1-\hat{y}_i)]$$

**å¤šåˆ†ç±»äº¤å‰ç†µ**ï¼š
$$L_{CE} = -\frac{1}{n}\sum_{i=1}^{n}\sum_{c=1}^{C}y_{i,c}\log(\hat{y}_{i,c})$$

**ç‰¹ç‚¹**ï¼š
- åŸºäºæœ€å¤§ä¼¼ç„¶ä¼°è®¡
- æƒ©ç½šé”™è¯¯é¢„æµ‹çš„ç½®ä¿¡åº¦
- ä¸softmaxç»“åˆæ•°å€¼ç¨³å®š

**æ•°å­¦æ¨å¯¼**ï¼š
åŸºäºä¼¯åŠªåˆ©åˆ†å¸ƒçš„è´Ÿå¯¹æ•°ä¼¼ç„¶ï¼š
$$P(y|x) = \hat{y}^y(1-\hat{y})^{1-y}$$
$$L = -\log P(y|x) = -[y\log\hat{y} + (1-y)\log(1-\hat{y})]$$

**ä»£ç å®ç°**ï¼š
```python
def cross_entropy_loss(y_true, y_pred, epsilon=1e-15):
    # æ•°å€¼ç¨³å®šæ€§å¤„ç†
    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)
    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))

def softmax_cross_entropy(logits, labels):
    # æ•°å€¼ç¨³å®šçš„softmax+äº¤å‰ç†µ
    shift_logits = logits - np.max(logits, axis=1, keepdims=True)
    exp_logits = np.exp(shift_logits)
    probs = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)
    return -np.mean(np.sum(labels * np.log(probs + 1e-15), axis=1))
```

#### Focal Loss
Focal loss æ˜¯åœ¨äº¤å‰ç†µå‰åŠ äº†ä¸€ä¸ª $(1-\hat{y})^\gamma$ å› å­ã€‚å®ƒçš„ä½œç”¨æ˜¯é™ä½å®¹æ˜“æ ·æœ¬çš„æƒé‡ï¼Œçªå‡ºå›°éš¾æ ·æœ¬çš„æ¢¯åº¦è´¡çŒ®ï¼Œä»è€Œåœ¨ç±»åˆ«æåº¦ä¸å¹³è¡¡çš„åœºæ™¯ä¸‹ï¼ˆæ¯”å¦‚ç›®æ ‡æ£€æµ‹ï¼‰è¡¨ç°æ›´å¥½ã€‚

**ä»€ä¹ˆæ˜¯å®¹æ˜“æ ·æœ¬å’Œå›°éš¾æ ·æœ¬**ï¼š
- **å®¹æ˜“æ ·æœ¬**ï¼šæ¨¡å‹é¢„æµ‹ç½®ä¿¡åº¦é«˜ä¸”æ­£ç¡®çš„æ ·æœ¬ï¼Œå³ $\hat{y}$ æ¥è¿‘1ï¼ˆæ­£æ ·æœ¬ï¼‰æˆ–æ¥è¿‘0ï¼ˆè´Ÿæ ·æœ¬ï¼‰
- **å›°éš¾æ ·æœ¬**ï¼šæ¨¡å‹é¢„æµ‹ç½®ä¿¡åº¦ä½æˆ–é”™è¯¯çš„æ ·æœ¬ï¼Œå³ $\hat{y}$ æ¥è¿‘0.5ï¼Œæ¨¡å‹æ— æ³•ç¡®å®šåˆ†ç±»

**Focal Lossçš„ä½œç”¨æœºåˆ¶**ï¼š
- å½“ $\hat{y} \rightarrow 1$ï¼ˆå®¹æ˜“æ ·æœ¬ï¼‰ï¼š$(1-\hat{y})^\gamma \rightarrow 0$ï¼ŒæŸå¤±æƒé‡é™ä½
- å½“ $\hat{y} \rightarrow 0.5$ï¼ˆå›°éš¾æ ·æœ¬ï¼‰ï¼š$(1-\hat{y})^\gamma \rightarrow 0.5^\gamma$ï¼Œä¿æŒè¾ƒé«˜æƒé‡
- å‚æ•° $\gamma$ æ§åˆ¶æƒé‡è¡°å‡é€Ÿåº¦ï¼Œ$\gamma$ è¶Šå¤§ï¼Œå¯¹å®¹æ˜“æ ·æœ¬çš„æƒé‡è¡°å‡è¶Šå¿«


$$L_{FL} = -\alpha(1-\hat{y})^\gamma \log(\hat{y})$$

**è®¾è®¡åŠ¨æœº**ï¼š
- è§£å†³ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜
- é™ä½æ˜“åˆ†ç±»æ ·æœ¬çš„æƒé‡
- èšç„¦äºå›°éš¾æ ·æœ¬

**å‚æ•°å«ä¹‰**ï¼š
- $\alpha$ï¼šç±»åˆ«æƒé‡ï¼Œå¹³è¡¡æ­£è´Ÿæ ·æœ¬
- $\gamma$ï¼šèšç„¦å‚æ•°ï¼Œæ§åˆ¶éš¾æ˜“æ ·æœ¬æƒé‡

**ä»£ç å®ç°**ï¼š
```python
def focal_loss(y_true, y_pred, alpha=0.25, gamma=2.0):
    epsilon = 1e-15
    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)
    
    # è®¡ç®—äº¤å‰ç†µ
    ce = -y_true * np.log(y_pred)
    
    # è®¡ç®—æƒé‡
    weight = alpha * y_true * (1 - y_pred) ** gamma
    
    return np.mean(weight * ce)
```

## æŸå¤±å‡½æ•°çš„å‡¸æ€§åˆ†æ

### å‡¸å‡½æ•°çš„é‡è¦æ€§

#### å®šä¹‰
å‡½æ•°$f$æ˜¯å‡¸å‡½æ•°å½“ä¸”ä»…å½“ï¼š
$$f(\lambda x_1 + (1-\lambda)x_2) \leq \lambda f(x_1) + (1-\lambda)f(x_2)$$

#### å‡¸æ€§çš„ä¼˜åŠ¿
- **å…¨å±€æœ€ä¼˜**ï¼šä»»ä½•å±€éƒ¨æœ€ä¼˜éƒ½æ˜¯å…¨å±€æœ€ä¼˜
- **æ”¶æ•›ä¿è¯**ï¼šæ¢¯åº¦ä¸‹é™ä¿è¯æ”¶æ•›åˆ°æœ€ä¼˜è§£
- **ä¼˜åŒ–æ•ˆç‡**ï¼šå‡¸ä¼˜åŒ–æœ‰æˆç†Ÿç†è®ºå’Œç®—æ³•

### å¸¸è§æŸå¤±å‡½æ•°çš„å‡¸æ€§åˆ†æ

#### å‡¸æŸå¤±å‡½æ•°
1. **å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰**ï¼š
   - $L(w) = ||Xw - y||^2$ å…³äº$w$æ˜¯å‡¸å‡½æ•°
   - äºŒæ¬¡å‹ï¼ŒHessiançŸ©é˜µåŠæ­£å®š

2. **å¹³å‡ç»å¯¹è¯¯å·®ï¼ˆMAEï¼‰**ï¼š
   - $L(w) = ||Xw - y||_1$ å…³äº$w$æ˜¯å‡¸å‡½æ•°
   - è™½ç„¶ä¸å¤„å¤„å¯å¾®ï¼Œä½†ä»æ˜¯å‡¸å‡½æ•°

3. **é€»è¾‘å›å½’æŸå¤±**ï¼š
   - $L(w) = \sum_i \log(1 + e^{-y_i w^T x_i})$ æ˜¯å‡¸å‡½æ•°
   - å¯¹æ•°å‡¸å‡½æ•°çš„çº¿æ€§ç»„åˆ

#### éå‡¸æŸå¤±å‡½æ•°
1. **ç¥ç»ç½‘ç»œæŸå¤±**ï¼š
   - ç”±äºéçº¿æ€§æ¿€æ´»å‡½æ•°ï¼Œæ•´ä½“éå‡¸
   - å­˜åœ¨å¤šä¸ªå±€éƒ¨æœ€ä¼˜è§£
   - éœ€è¦ç‰¹æ®Šåˆå§‹åŒ–å’Œä¼˜åŒ–ç­–ç•¥

2. **0-1æŸå¤±**ï¼š
   - $L(y, \hat{y}) = \mathbb{I}[y \neq \hat{y}]$
   - éå‡¸ä¸”ä¸è¿ç»­
   - é€šå¸¸ç”¨å‡¸ä»£ç†æŸå¤±å‡½æ•°æ›¿ä»£

## æ­£åˆ™åŒ–é¡¹çš„ä½œç”¨æœºåˆ¶
### æ­£åˆ™åŒ–çš„æœ¬è´¨ä¸ä½œç”¨

#### ä»€ä¹ˆæ˜¯æ­£åˆ™åŒ–ï¼Ÿ
æ­£åˆ™åŒ–ï¼ˆRegularizationï¼‰æ˜¯ä¸€ç§é˜²æ­¢æ¨¡å‹è¿‡æ‹Ÿåˆçš„æŠ€æœ¯ï¼Œé€šè¿‡åœ¨æŸå¤±å‡½æ•°ä¸­æ·»åŠ é¢å¤–çš„çº¦æŸé¡¹æ¥é™åˆ¶æ¨¡å‹å¤æ‚åº¦ã€‚

**æ ¸å¿ƒæ€æƒ³**ï¼š
- **åå·®-æ–¹å·®æƒè¡¡**ï¼šå¢åŠ ä¸€ç‚¹åå·®æ¥æ˜¾è‘—å‡å°‘æ–¹å·®
- **å¥¥å¡å§†å‰ƒåˆ€åŸç†**ï¼šç®€å•æ¨¡å‹ä¼˜äºå¤æ‚æ¨¡å‹
- **ç»“æ„é£é™©æœ€å°åŒ–**ï¼šåŒæ—¶è€ƒè™‘ç»éªŒé£é™©å’Œæ¨¡å‹å¤æ‚åº¦

#### ä¸ºä»€ä¹ˆéœ€è¦æ­£åˆ™åŒ–ï¼Ÿ

**è¿‡æ‹Ÿåˆé—®é¢˜**ï¼š
- æ¨¡å‹åœ¨è®­ç»ƒé›†ä¸Šè¡¨ç°å¾ˆå¥½ï¼Œä½†åœ¨æµ‹è¯•é›†ä¸Šè¡¨ç°å·®
- æ¨¡å‹å­¦ä¹ äº†å™ªå£°è€ŒéçœŸå®çš„æ•°æ®æ¨¡å¼
- å‚æ•°æ•°é‡è¿œå¤§äºè®­ç»ƒæ ·æœ¬æ•°é‡æ—¶å°¤å…¶ä¸¥é‡

**æ­£åˆ™åŒ–çš„ä½œç”¨æœºåˆ¶**ï¼š
1. **é™åˆ¶å‚æ•°ç©ºé—´**ï¼šç¼©å°å¯è¡Œè§£çš„èŒƒå›´
2. **å¹³æ»‘å†³ç­–è¾¹ç•Œ**ï¼šé¿å…è¿‡äºå¤æ‚çš„å†³ç­–å‡½æ•°
3. **æé«˜æ³›åŒ–èƒ½åŠ›**ï¼šå‡å°‘å¯¹è®­ç»ƒæ•°æ®çš„è¿‡åº¦ä¾èµ–
4. **æ•°å€¼ç¨³å®šæ€§**ï¼šæ”¹å–„ä¼˜åŒ–é—®é¢˜çš„æ¡ä»¶æ•°

#### æ­£åˆ™åŒ–çš„ç›´è§‚ç†è§£

**ä¿¡æ¯è®ºè§’åº¦**ï¼š
- æ­£åˆ™åŒ–å®é™…ä¸Šæ˜¯åœ¨ç¼–ç æ¨¡å‹å‚æ•°
- æ›´ç®€å•çš„æ¨¡å‹éœ€è¦æ›´å°‘çš„æ¯”ç‰¹æ¥æè¿°
- ç¬¦åˆæœ€å°æè¿°é•¿åº¦åŸç†

**è´å¶æ–¯è§’åº¦**ï¼š
- æ­£åˆ™åŒ–é¡¹ç›¸å½“äºå‚æ•°çš„å…ˆéªŒåˆ†å¸ƒ
- L2æ­£åˆ™åŒ– â†” é«˜æ–¯å…ˆéªŒ
- L1æ­£åˆ™åŒ– â†” æ‹‰æ™®æ‹‰æ–¯å…ˆéªŒ

**å‡ ä½•è§’åº¦**ï¼š
- åœ¨å‚æ•°ç©ºé—´ä¸­å®šä¹‰çº¦æŸåŒºåŸŸ
- å¯»æ‰¾æ•°æ®æ‹Ÿåˆä¸çº¦æŸæ»¡è¶³çš„å¹³è¡¡ç‚¹
- ä¸åŒæ­£åˆ™åŒ–å¯¹åº”ä¸åŒå½¢çŠ¶çš„çº¦æŸåŒºåŸŸ

### æ­£åˆ™åŒ–çš„æ•°å­¦å½¢å¼
$$L_{total} = L_{data} + \lambda R(w)$$

å…¶ä¸­ï¼š
- $L_{data}$ï¼šæ•°æ®é¡¹ï¼ˆæ‹ŸåˆæŸå¤±ï¼‰
- $R(w)$ï¼šæ­£åˆ™åŒ–é¡¹
- $\lambda$ï¼šæ­£åˆ™åŒ–å¼ºåº¦

### å¸¸è§æ­£åˆ™åŒ–ç±»å‹

#### L1æ­£åˆ™åŒ–ï¼ˆLassoï¼‰
$$R_{L1}(w) = ||w||_1 = \sum_i |w_i|$$

**ç‰¹ç‚¹**ï¼š
- ç¨€ç–æ€§ï¼šäº§ç”Ÿç¨€ç–æƒé‡
- ç‰¹å¾é€‰æ‹©ï¼šè‡ªåŠ¨å»é™¤ä¸é‡è¦ç‰¹å¾
- éå…‰æ»‘ï¼šåœ¨0å¤„ä¸å¯å¾®

**å‡ ä½•è§£é‡Š**ï¼š
- çº¦æŸåŒºåŸŸæ˜¯è±å½¢
- å®¹æ˜“åœ¨åæ ‡è½´ä¸Šå–åˆ°æœ€ä¼˜è§£

**ä»£ç å®ç°**ï¼š
```python
def l1_regularization(weights, lambda_reg):
    return lambda_reg * np.sum(np.abs(weights))

def l1_gradient(weights, lambda_reg):
    return lambda_reg * np.sign(weights)
```

#### L2æ­£åˆ™åŒ–ï¼ˆRidgeï¼‰
$$R_{L2}(w) = ||w||_2^2 = \sum_i w_i^2$$

**ç‰¹ç‚¹**ï¼š
- æƒé‡è¡°å‡ï¼šé˜²æ­¢æƒé‡è¿‡å¤§
- å¹³æ»‘è§£ï¼šå€¾å‘äºå¹³å‡åˆ†é…æƒé‡
- å¤„å¤„å¯å¾®ï¼šä¼˜åŒ–å‹å¥½

**å‡ ä½•è§£é‡Š**ï¼š
- çº¦æŸåŒºåŸŸæ˜¯åœ†å½¢
- è§£é€šå¸¸ä¸åœ¨åæ ‡è½´ä¸Š

**è´å¶æ–¯è§£é‡Š**ï¼š
L2æ­£åˆ™åŒ–ç­‰ä»·äºç»™æƒé‡åŠ é«˜æ–¯å…ˆéªŒï¼š
$$p(w) = \mathcal{N}(0, \frac{1}{\lambda}I)$$

#### å¼¹æ€§ç½‘ç»œï¼ˆElastic Netï¼‰
$$R_{EN}(w) = \alpha ||w||_1 + (1-\alpha)||w||_2^2$$

ç»“åˆL1å’ŒL2çš„ä¼˜ç‚¹ï¼š
- ç¨€ç–æ€§ + åˆ†ç»„æ•ˆåº”
- ç›¸å…³ç‰¹å¾å€¾å‘äºä¸€èµ·é€‰æ‹©æˆ–ä¸¢å¼ƒ

## å¤šä»»åŠ¡å­¦ä¹ ä¸­çš„æŸå¤±å‡½æ•°è®¾è®¡

### å¤šä»»åŠ¡æŸå¤±çš„ä¸€èˆ¬å½¢å¼
$$L_{multi} = \sum_{t=1}^{T} \alpha_t L_t(f_t(x), y_t)$$

å…¶ä¸­$T$æ˜¯ä»»åŠ¡æ•°ï¼Œ$\alpha_t$æ˜¯ä»»åŠ¡æƒé‡ã€‚

### ä»»åŠ¡æƒé‡è®¾è®¡ç­–ç•¥

#### 1. é™æ€æƒé‡
- **å‡ç­‰æƒé‡**ï¼š$\alpha_t = 1/T$
- **æ‰‹åŠ¨è°ƒèŠ‚**ï¼šåŸºäºä»»åŠ¡é‡è¦æ€§
- **ä¸ç¡®å®šæ€§æƒé‡**ï¼šåŸºäºä»»åŠ¡éš¾åº¦

#### 2. åŠ¨æ€æƒé‡

**æ¢¯åº¦å½’ä¸€åŒ–ï¼ˆGradNormï¼‰**ï¼š
æ ¹æ®æ¢¯åº¦ç›¸å¯¹å˜åŒ–ç‡è°ƒæ•´æƒé‡ï¼š
$$\alpha_t^{(i+1)} = \alpha_t^{(i)} \cdot \left(\frac{r_t^{(i)}}{\bar{r}^{(i)}}\right)^{\beta}$$

**è‡ªé€‚åº”æƒé‡**ï¼š
```python
class AdaptiveWeights:
    def __init__(self, num_tasks, alpha=0.16):
        self.num_tasks = num_tasks
        self.alpha = alpha
        self.weights = np.ones(num_tasks)
        
    def update(self, losses):
        # åŸºäºæŸå¤±å˜åŒ–ç‡æ›´æ–°æƒé‡
        loss_ratios = losses / np.mean(losses)
        self.weights *= np.exp(-self.alpha * loss_ratios)
        self.weights /= np.sum(self.weights)  # å½’ä¸€åŒ–
```

#### 3. ä¸ç¡®å®šæ€§åŠ æƒ
åŸºäºåŒæ–¹å·®ä¸ç¡®å®šæ€§çš„å¤šä»»åŠ¡æŸå¤±ï¼š
$$L = \sum_{t=1}^{T} \frac{1}{2\sigma_t^2} L_t + \log \sigma_t$$

## æ€»ç»“

æŸå¤±å‡½æ•°è®¾è®¡æ˜¯æ·±åº¦å­¦ä¹ çš„æ ¸å¿ƒï¼Œé¢è¯•ä¸­éœ€è¦æŒæ¡ï¼š

### ğŸ¯ **æ ¸å¿ƒè¦ç‚¹**
1. **ä»»åŠ¡é€‚é…**ï¼šä¸åŒä»»åŠ¡é€‰æ‹©åˆé€‚çš„æŸå¤±å‡½æ•°
2. **æ•°å­¦ç†è§£**ï¼šæŒæ¡æŸå¤±å‡½æ•°çš„æ•°å­¦åŸç†å’Œæ¢¯åº¦è®¡ç®—
3. **å‡¸æ€§åˆ†æ**ï¼šç†è§£å‡¸æ€§å¯¹ä¼˜åŒ–çš„å½±å“
4. **æ­£åˆ™åŒ–**ï¼šæŒæ¡å„ç§æ­£åˆ™åŒ–æŠ€æœ¯çš„åŸç†å’Œåº”ç”¨
5. **å¤šä»»åŠ¡å­¦ä¹ **ï¼šç†è§£å¤šä»»åŠ¡æŸå¤±å‡½æ•°çš„è®¾è®¡ç­–ç•¥

### ğŸ“ **é¢è¯•å‡†å¤‡**
- èƒ½å¤Ÿè§£é‡Šä¸ºä»€ä¹ˆç‰¹å®šä»»åŠ¡é€‰æ‹©ç‰¹å®šæŸå¤±å‡½æ•°
- ç†è§£æŸå¤±å‡½æ•°çš„æ•°å€¼ç¨³å®šæ€§é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ
- æŒæ¡æ­£åˆ™åŒ–çš„æ•°å­¦åŸç†å’Œå®é™…æ•ˆæœ
- èƒ½å¤Ÿè®¾è®¡è‡ªå®šä¹‰æŸå¤±å‡½æ•°è§£å†³ç‰¹å®šé—®é¢˜
- äº†è§£å¤šä»»åŠ¡å­¦ä¹ ä¸­çš„æƒé‡å¹³è¡¡ç­–ç•¥

**å…³é”®è®°å¿†ç‚¹**ï¼š
- å›å½’ç”¨MSE/MAEï¼Œåˆ†ç±»ç”¨äº¤å‰ç†µ
- äº¤å‰ç†µæ¯”MSEæ›´é€‚åˆåˆ†ç±»çš„æ•°å­¦åŸå› 
- L1äº§ç”Ÿç¨€ç–æ€§ï¼ŒL2é˜²æ­¢è¿‡æ‹Ÿåˆ
- å¤šä»»åŠ¡å­¦ä¹ éœ€è¦å¹³è¡¡ä¸åŒä»»åŠ¡çš„è´¡çŒ®

---

## ç›¸å…³ç¬”è®°
<!-- è‡ªåŠ¨ç”Ÿæˆ -->

æš‚æ— ç›¸å…³ç¬”è®°

