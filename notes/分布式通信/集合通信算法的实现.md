---
created: '2025-10-19'
last_reviewed: null
next_review: '2025-10-19'
review_count: 0
difficulty: medium
mastery_level: 0.0
tags:
- 分布式通信
- 分布式通信/集合通信算法的实现.md
related_outlines: []
---
# 集合通信算法的实现

## All-Reduce算法实现

### Ring All-Reduce算法
```python
import torch
import torch.distributed as dist

class RingAllReduce:
    def __init__(self, world_size, rank):
        self.world_size = world_size
        self.rank = rank
        self.left_neighbor = (rank - 1) % world_size
        self.right_neighbor = (rank + 1) % world_size
    
    def allreduce(self, tensor):
        """Ring All-Reduce实现"""
        # Phase 1: Reduce-Scatter
        # 将tensor分成world_size个chunk
        chunk_size = tensor.numel() // self.world_size
        chunks = tensor.split(chunk_size)
        
        for step in range(self.world_size - 1):
            # 计算发送和接收的chunk索引
            send_chunk_idx = (self.rank - step) % self.world_size
            recv_chunk_idx = (self.rank - step - 1) % self.world_size
            
            # 发送数据到右邻居
            send_req = dist.isend(chunks[send_chunk_idx], 
                                 dst=self.right_neighbor)
            
            # 从左邻居接收数据
            recv_tensor = torch.zeros_like(chunks[recv_chunk_idx])
            recv_req = dist.irecv(recv_tensor, 
                                 src=self.left_neighbor)
            
            # 等待通信完成
            send_req.wait()
            recv_req.wait()
            
            # 累加接收到的数据
            chunks[recv_chunk_idx] += recv_tensor
        
        # Phase 2: All-Gather
        for step in range(self.world_size - 1):
            send_chunk_idx = (self.rank - step + 1) % self.world_size
            recv_chunk_idx = (self.rank - step) % self.world_size
            
            # 发送已经reduce的chunk
            send_req = dist.isend(chunks[send_chunk_idx], 
                                 dst=self.right_neighbor)
            
            # 接收其他节点reduce的chunk
            recv_req = dist.irecv(chunks[recv_chunk_idx], 
                                 src=self.left_neighbor)
            
            send_req.wait()
            recv_req.wait()
        
        return torch.cat(chunks)
```

### Tree All-Reduce算法
```python
class TreeAllReduce:
    def __init__(self, world_size, rank):
        self.world_size = world_size
        self.rank = rank
        self.tree = self.build_binary_tree()
    
    def build_binary_tree(self):
        """构建二叉树结构"""
        tree = {}
        for i in range(self.world_size):
            left_child = 2 * i + 1
            right_child = 2 * i + 2
            parent = (i - 1) // 2 if i > 0 else None
            
            tree[i] = {
                'parent': parent,
                'left_child': left_child if left_child < self.world_size else None,
                'right_child': right_child if right_child < self.world_size else None,
                'children': []
            }
            
            if tree[i]['left_child'] is not None:
                tree[i]['children'].append(tree[i]['left_child'])
            if tree[i]['right_child'] is not None:
                tree[i]['children'].append(tree[i]['right_child'])
        
        return tree
    
    def allreduce(self, tensor):
        """Tree All-Reduce实现"""
        # Phase 1: 从叶子节点向根节点reduce
        self.reduce_up(tensor.clone())
        
        # Phase 2: 从根节点向叶子节点broadcast
        result = self.broadcast_down(tensor.clone())
        
        return result
    
    def reduce_up(self, tensor):
        """向上reduce阶段"""
        node_info = self.tree[self.rank]
        
        # 如果有子节点，等待子节点的数据
        if node_info['children']:
            for child in node_info['children']:
                child_data = torch.zeros_like(tensor)
                dist.recv(child_data, src=child)
                tensor += child_data
        
        # 如果有父节点，发送数据给父节点
        if node_info['parent'] is not None:
            dist.send(tensor, dst=node_info['parent'])
    
    def broadcast_down(self, tensor):
        """向下broadcast阶段"""
        node_info = self.tree[self.rank]
        
        # 如果有父节点，从父节点接收结果
        if node_info['parent'] is not None:
            dist.recv(tensor, src=node_info['parent'])
        
        # 如果有子节点，发送结果给子节点
        if node_info['children']:
            for child in node_info['children']:
                dist.send(tensor, dst=child)
        
        return tensor
```

### Hierarchical All-Reduce算法
```python
class HierarchicalAllReduce:
    def __init__(self, world_size, rank, nodes_per_group=8):
        self.world_size = world_size
        self.rank = rank
        self.nodes_per_group = nodes_per_group
        
        # 计算组内和组间的rank
        self.local_rank = rank % nodes_per_group
        self.group_rank = rank // nodes_per_group
        self.num_groups = (world_size + nodes_per_group - 1) // nodes_per_group
        
        # 创建通信组
        self.create_communication_groups()
    
    def create_communication_groups(self):
        """创建层次化通信组"""
        # 组内通信组
        local_group_ranks = []
        for i in range(self.nodes_per_group):
            global_rank = self.group_rank * self.nodes_per_group + i
            if global_rank < self.world_size:
                local_group_ranks.append(global_rank)
        
        self.local_group = dist.new_group(local_group_ranks)
        
        # 组间通信组（每组的代表节点）
        if self.local_rank == 0:  # 组代表
            inter_group_ranks = []
            for g in range(self.num_groups):
                representative = g * self.nodes_per_group
                if representative < self.world_size:
                    inter_group_ranks.append(representative)
            
            self.inter_group = dist.new_group(inter_group_ranks)
        else:
            self.inter_group = None
    
    def allreduce(self, tensor):
        """层次化All-Reduce实现"""
        # Step 1: 组内All-Reduce
        dist.all_reduce(tensor, group=self.local_group)
        
        # Step 2: 组代表进行组间All-Reduce
        if self.inter_group is not None:
            dist.all_reduce(tensor, group=self.inter_group)
        
        # Step 3: 组代表将结果广播给组内其他节点
        dist.broadcast(tensor, src=self.group_rank * self.nodes_per_group, 
                      group=self.local_group)
        
        return tensor
```

## Broadcast算法实现

### Binomial Tree Broadcast
```python
class BinomialBroadcast:
    def __init__(self, world_size, rank):
        self.world_size = world_size
        self.rank = rank
    
    def broadcast(self, tensor, root=0):
        """二项树广播实现"""
        if self.rank == root:
            # 根节点发送数据
            self.send_phase(tensor, root)
        else:
            # 非根节点接收数据
            tensor = self.receive_phase(tensor, root)
        
        return tensor
    
    def send_phase(self, tensor, root):
        """发送阶段"""
        relative_rank = (self.rank - root + self.world_size) % self.world_size
        
        # 计算需要发送的轮数
        mask = 1
        while mask < self.world_size:
            if relative_rank & mask:
                # 当前节点在这一轮不发送
                break
            
            # 计算目标节点
            target_relative = relative_rank | mask
            if target_relative < self.world_size:
                target = (target_relative + root) % self.world_size
                dist.send(tensor, dst=target)
            
            mask <<= 1
    
    def receive_phase(self, tensor, root):
        """接收阶段"""
        relative_rank = (self.rank - root + self.world_size) % self.world_size
        
        # 找到发送者
        mask = 1
        while mask < self.world_size:
            if relative_rank & mask:
                # 从发送者接收数据
                sender_relative = relative_rank & (~mask)
                sender = (sender_relative + root) % self.world_size
                dist.recv(tensor, src=sender)
                break
            mask <<= 1
        
        # 继续向下传播
        self.send_phase(tensor, root)
        return tensor
```

### Pipeline Broadcast
```python
class PipelineBroadcast:
    def __init__(self, world_size, rank, pipeline_depth=4):
        self.world_size = world_size
        self.rank = rank
        self.pipeline_depth = pipeline_depth
    
    def broadcast(self, tensor, root=0):
        """流水线广播实现"""
        # 将tensor分成多个段进行流水线传输
        chunk_size = tensor.numel() // self.pipeline_depth
        chunks = tensor.split(chunk_size)
        
        if self.rank == root:
            self.pipeline_send(chunks)
        else:
            chunks = self.pipeline_receive(chunks)
        
        return torch.cat(chunks)
    
    def pipeline_send(self, chunks):
        """流水线发送"""
        next_rank = (self.rank + 1) % self.world_size
        
        for i, chunk in enumerate(chunks):
            # 异步发送当前chunk
            if next_rank != self.rank:  # 避免自发自收
                send_req = dist.isend(chunk, dst=next_rank)
                send_req.wait()
    
    def pipeline_receive(self, chunks):
        """流水线接收"""
        prev_rank = (self.rank - 1 + self.world_size) % self.world_size
        next_rank = (self.rank + 1) % self.world_size
        
        received_chunks = []
        
        for i in range(len(chunks)):
            # 接收前一个节点的数据
            chunk = torch.zeros_like(chunks[i])
            recv_req = dist.irecv(chunk, src=prev_rank)
            recv_req.wait()
            
            received_chunks.append(chunk)
            
            # 转发给下一个节点
            if next_rank != prev_rank:  # 避免环路
                send_req = dist.isend(chunk, dst=next_rank)
                send_req.wait()
        
        return received_chunks
```

## Gather/Scatter算法实现

### All-to-All算法
```python
class AllToAll:
    def __init__(self, world_size, rank):
        self.world_size = world_size
        self.rank = rank
    
    def all_to_all(self, send_tensors):
        """All-to-All通信实现"""
        assert len(send_tensors) == self.world_size
        
        recv_tensors = [torch.zeros_like(send_tensors[0]) 
                       for _ in range(self.world_size)]
        
        # 创建发送和接收请求
        send_reqs = []
        recv_reqs = []
        
        for i in range(self.world_size):
            if i != self.rank:
                # 发送请求
                send_req = dist.isend(send_tensors[i], dst=i)
                send_reqs.append(send_req)
                
                # 接收请求
                recv_req = dist.irecv(recv_tensors[i], src=i)
                recv_reqs.append(recv_req)
            else:
                # 本地拷贝
                recv_tensors[i] = send_tensors[i].clone()
        
        # 等待所有通信完成
        for req in send_reqs + recv_reqs:
            req.wait()
        
        return recv_tensors
```

## 优化技术实现

### 通信与计算重叠
```python
class OverlappedAllReduce:
    def __init__(self, world_size, rank):
        self.world_size = world_size
        self.rank = rank
        self.communication_stream = torch.cuda.Stream()
    
    def overlapped_allreduce(self, tensor, computation_fn=None):
        """通信与计算重叠的All-Reduce"""
        # 分割tensor为多个chunk
        num_chunks = 4
        chunk_size = tensor.numel() // num_chunks
        chunks = tensor.split(chunk_size)
        
        results = []
        
        for i, chunk in enumerate(chunks):
            # 在通信流中启动All-Reduce
            with torch.cuda.stream(self.communication_stream):
                result_chunk = chunk.clone()
                dist.all_reduce(result_chunk)
                results.append(result_chunk)
            
            # 在默认流中进行计算（如果有）
            if computation_fn and i > 0:
                # 可以使用前一个chunk的结果进行计算
                computation_fn(results[i-1])
        
        # 等待所有通信完成
        torch.cuda.current_stream().wait_stream(self.communication_stream)
        
        return torch.cat(results)
```

### 自适应算法选择
```python
class AdaptiveCollectives:
    def __init__(self, world_size, rank):
        self.world_size = world_size
        self.rank = rank
        self.performance_history = {}
        self.algorithms = {
            'ring': RingAllReduce(world_size, rank),
            'tree': TreeAllReduce(world_size, rank),
            'hierarchical': HierarchicalAllReduce(world_size, rank)
        }
    
    def adaptive_allreduce(self, tensor):
        """自适应选择All-Reduce算法"""
        message_size = tensor.numel() * tensor.element_size()
        
        # 选择算法
        algorithm = self.select_algorithm(message_size)
        
        # 执行并测量性能
        start_time = time.time()
        result = self.algorithms[algorithm].allreduce(tensor)
        end_time = time.time()
        
        # 更新性能历史
        self.update_performance_history(algorithm, message_size, 
                                      end_time - start_time)
        
        return result
    
    def select_algorithm(self, message_size):
        """根据消息大小和历史性能选择算法"""
        if message_size < 64 * 1024:  # 64KB
            return 'tree'  # 小消息用树算法
        elif message_size > 16 * 1024 * 1024:  # 16MB
            return 'ring'  # 大消息用环算法
        elif self.world_size > 64:
            return 'hierarchical'  # 大规模用层次算法
        else:
            # 基于历史性能选择
            return self.select_based_on_history(message_size)
    
    def select_based_on_history(self, message_size):
        """基于历史性能选择算法"""
        best_algorithm = 'ring'
        best_time = float('inf')
        
        for algo in self.algorithms.keys():
            if algo in self.performance_history:
                # 找到最接近的消息大小的记录
                times = self.performance_history[algo]
                if times:
                    avg_time = sum(times) / len(times)
                    if avg_time < best_time:
                        best_time = avg_time
                        best_algorithm = algo
        
        return best_algorithm
```

## 性能分析与调优

### 通信模式分析
```python
class CommunicationProfiler:
    def __init__(self):
        self.communication_log = []
    
    def profile_collective(self, collective_fn, tensor, *args, **kwargs):
        """分析集合通信性能"""
        message_size = tensor.numel() * tensor.element_size()
        
        # 预热
        for _ in range(5):
            collective_fn(tensor.clone(), *args, **kwargs)
        
        torch.cuda.synchronize()
        
        # 正式测量
        times = []
        for _ in range(20):
            start_time = time.time()
            result = collective_fn(tensor.clone(), *args, **kwargs)
            torch.cuda.synchronize()
            end_time = time.time()
            times.append(end_time - start_time)
        
        # 统计信息
        avg_time = sum(times) / len(times)
        bandwidth = message_size / avg_time / (1024**3)  # GB/s
        
        profile_data = {
            'message_size': message_size,
            'avg_time': avg_time,
            'bandwidth': bandwidth,
            'min_time': min(times),
            'max_time': max(times),
            'std_time': statistics.stdev(times)
        }
        
        self.communication_log.append(profile_data)
        return result, profile_data
```

---

## 相关笔记
<!-- 自动生成 -->

暂无相关笔记

