---
created: '2025-10-19'
last_reviewed: '2025-11-03'
next_review: '2025-11-05'
review_count: 1
difficulty: medium
mastery_level: 0.15
tags:
- cuda
- cuda/如何选择合适的线程块大小和分块大小？.md
related_outlines: []
---
# 如何选择合适的线程块大小和分块大小？

## 面试标准答案

选择合适的线程块大小和分块大小需要权衡**占用率、资源限制和访存效率**。线程块大小选择要考虑：1) **warp倍数** - 必须是32的倍数，典型值128-256；2) **占用率** - 每个SM至少2-4个block以隐藏延迟；3) **资源限制** - 寄存器和共享内存使用量。分块大小（Tile）选择要考虑：1) **共享内存容量** - 通常16×16到64×64；2) **数据复用** - Tile越大复用率越高；3) **bank conflict** - 行列维度避免32的倍数。实践中，线程块常用256（16×16）或128（32×4），Tile大小常用32×32或64×64，通过实验找到最优配置。

---

## 详细讲解

### 1. 基础概念

#### 1.1 关键参数定义

**线程块大小（Block Size）**：
```cuda
dim3 blockDim(BX, BY, BZ);  // BX × BY × BZ 个线程
int threads_per_block = BX * BY * BZ;
```

**分块大小（Tile Size）**：
```cuda
#define TILE_M 64  // 矩阵M维度的tile
#define TILE_N 64  // 矩阵N维度的tile
#define TILE_K 8   // 矩阵K维度的tile

__shared__ float As[TILE_M][TILE_K];
__shared__ float Bs[TILE_K][TILE_N];
```

**关系**：
- 线程块大小 ≤ Tile 覆盖的元素数
- 通常：`threads_per_block = (TILE_M / THREAD_TILE_M) × (TILE_N / THREAD_TILE_N)`

#### 1.2 GPU 硬件限制

**Ampere (A100) 规格**：
```
每个SM最大线程数: 2048
每个SM最大线程块数: 32
每个线程块最大线程数: 1024
每个SM寄存器: 65536
每个SM共享内存: 164 KB (可配置)
Warp大小: 32
```

### 2. 线程块大小选择

#### 2.1 基本原则

**规则1：必须是Warp的倍数（32）**
```cuda
// ✓ 好的选择
dim3 blockDim(32, 1);     // 32
dim3 blockDim(16, 16);    // 256
dim3 blockDim(32, 8);     // 256
dim3 blockDim(32, 4);     // 128

// ✗ 不好的选择
dim3 blockDim(17, 15);    // 255，不是32的倍数
dim3 blockDim(31, 1);     // 31，浪费warp资源
```

**原因**：GPU以warp（32线程）为调度单位，不足32的warp会浪费资源。

**规则2：典型范围128-256**
```cuda
// 推荐的配置
dim3 blockDim(128);       // 4个warp
dim3 blockDim(256);       // 8个warp
dim3 blockDim(512);       // 16个warp（某些情况）

// 不太推荐
dim3 blockDim(64);        // 太小，占用率可能不足
dim3 blockDim(1024);      // 太大，资源限制严重
```

#### 2.2 占用率计算

**占用率（Occupancy）**：
```
实际活跃warp数
─────────────────  × 100%
理论最大warp数
```

**计算示例**（A100）：
```cuda
// 配置1: 256 threads/block
每个SM最多线程: 2048
每个SM最多block: 32

理论block数 = min(2048/256, 32) = min(8, 32) = 8
活跃warp数 = 8 × (256/32) = 64
最大warp数 = 2048/32 = 64
占用率 = 64/64 = 100%
```

```cuda
// 配置2: 128 threads/block
理论block数 = min(2048/128, 32) = min(16, 32) = 16
活跃warp数 = 16 × (128/32) = 64
占用率 = 64/64 = 100%
```

**受资源限制的情况**：
```cuda
// 假设每个线程使用64个寄存器
// 配置: 256 threads/block

寄存器限制的block数 = 65536 / (256 × 64) = 4
实际block数 = min(8, 4) = 4
活跃warp数 = 4 × 8 = 32
占用率 = 32/64 = 50%  // 受寄存器限制
```

#### 2.3 使用Occupancy Calculator

```cuda
// CUDA提供的API
int minGridSize, blockSize;
cudaOccupancyMaxPotentialBlockSize(
    &minGridSize,
    &blockSize,
    kernel_function,
    0,  // 动态共享内存大小
    0   // 最大block大小限制（0表示无限制）
);

printf("Recommended block size: %d\n", blockSize);
```

**命令行工具**：
```bash
# 使用CUDA Occupancy Calculator
nvcc --ptxas-options=-v kernel.cu

# 输出示例：
# ptxas info : Used 32 registers, 8192 bytes smem
# ptxas info : Compiling entry function '_kernel' for 'sm_80'
```

### 3. 分块大小（Tile Size）选择

#### 3.1 共享内存限制

**计算共享内存使用**：
```cuda
#define TILE_SIZE 32
__shared__ float As[TILE_SIZE][TILE_SIZE + 1];
__shared__ float Bs[TILE_SIZE][TILE_SIZE + 1];

// 共享内存使用
int smem_per_block = 2 × TILE_SIZE × (TILE_SIZE + 1) × sizeof(float)
                   = 2 × 32 × 33 × 4 
                   = 8,448 bytes
                   ≈ 8.25 KB
```

**不同Tile大小的共享内存使用**：

| Tile Size | 共享内存(KB) | A100 每SM可容纳Block数 |
| --------- | ------------ | ---------------------- |
| 16×16     | 2.1          | 78 (受线程数限制)      |
| 32×32     | 8.25         | 19                     |
| 64×64     | 33           | 4                      |
| 128×128   | 132          | 1                      |

#### 3.2 数据复用率

**分析**：
```
C[i][j] = Σ(k) A[i][k] × B[k][j]

单个元素计算需要读取：
- A的一行: K个元素
- B的一列: K个元素
- 总计: 2K次读取

使用Tile（TILE_SIZE × TILE_SIZE）：
- 加载A的Tile: TILE_SIZE × TILE_SIZE 个元素
- 加载B的Tile: TILE_SIZE × TILE_SIZE 个元素
- 计算: TILE_SIZE³ 次乘加
- 复用率: TILE_SIZE 倍
```

**Tile越大，复用率越高**：
```
TILE=16: 16倍复用
TILE=32: 32倍复用
TILE=64: 64倍复用
```

**但受限于共享内存和占用率**。

#### 3.3 矩形Tile

**非正方形Tile**：
```cuda
#define TILE_M 128
#define TILE_N 128
#define TILE_K 8   // K维度较小

__shared__ float As[TILE_M][TILE_K + 1];
__shared__ float Bs[TILE_K][TILE_N + 1];

// 共享内存: (128×9 + 8×129) × 4 = 8.7 KB
```

**优势**：
- 减少共享内存使用
- 适应访存带宽和计算的比例
- 更灵活的配置

#### 3.4 Bank Conflict考虑

**避免32的倍数**：
```cuda
// ✗ 可能有bank conflict
__shared__ float As[32][32];

// ✓ 添加padding
__shared__ float As[32][33];  // +1

// ✓ 使用非32倍数
__shared__ float As[31][31];
__shared__ float As[36][36];
```

### 4. 实践中的配置

#### 4.1 GEMM常用配置

**小矩阵（< 512）**：
```cuda
// 配置1
#define TILE_SIZE 16
dim3 blockDim(16, 16);  // 256 threads

// 配置2
#define TILE_SIZE 32
dim3 blockDim(8, 8);    // 64 threads，每个线程计算4×4
```

**中等矩阵（512-2048）**：
```cuda
// 配置1
#define TILE_SIZE 32
dim3 blockDim(16, 16);  // 256 threads

// 配置2（寄存器分块）
#define TILE_SIZE 64
#define THREAD_TILE 8
dim3 blockDim(8, 8);    // 64 threads，每个线程计算8×8
```

**大矩阵（> 2048）**：
```cuda
// 配置1（Tensor Core）
ThreadblockShape<128, 128, 32>
WarpShape<64, 64, 32>
dim3 blockDim(256);

// 配置2（优化的SIMT）
#define TILE_M 128
#define TILE_N 128
#define TILE_K 8
dim3 blockDim(256);
```

#### 4.2 不同优化阶段的配置

**阶段1：基础Tiling**
```cuda
#define TILE_SIZE 32
dim3 blockDim(32, 32);  // 1024 threads（可能太多）
```

**阶段2：添加寄存器分块**
```cuda
#define TILE_SIZE 32
#define THREAD_TILE 4
dim3 blockDim(8, 8);    // 64 threads
// 每个线程计算4×4=16个元素
```

**阶段3：双缓冲**
```cuda
#define TILE_SIZE 32
#define STAGES 2
// 共享内存翻倍，可能需要减小Tile或增加threads
dim3 blockDim(16, 16);  // 256 threads
```

**阶段4：Tensor Core**
```cuda
#define TILE_M 64
#define TILE_N 64
#define WMMA_M 16
#define WMMA_N 16
dim3 blockDim(128);  // 4个warp，每个warp一个WMMA
```

### 5. 调优方法

#### 5.1 系统性调优

**步骤1：确定搜索空间**
```python
tile_sizes = [16, 32, 48, 64, 96, 128]
block_sizes = [64, 128, 256, 512]
thread_tiles = [4, 8, 16]

for tile in tile_sizes:
    for block in block_sizes:
        for ttile in thread_tiles:
            if is_valid_config(tile, block, ttile):
                benchmark(tile, block, ttile)
```

**步骤2：验证配置有效性**
```cuda
bool is_valid_config(int tile, int block, int thread_tile) {
    // 检查1: 共享内存
    int smem = 2 * tile * tile * sizeof(float);
    if (smem > 48 * 1024) return false;  // 48KB限制
    
    // 检查2: 线程数
    if (block > 1024) return false;
    if (block % 32 != 0) return false;
    
    // 检查3: 线程和Tile的关系
    int threads_needed = (tile / thread_tile) * (tile / thread_tile);
    if (threads_needed > block) return false;
    
    return true;
}
```

**步骤3：性能测试**
```cuda
// benchmark.cu
template<int TILE_SIZE, int BLOCK_SIZE>
void benchmark() {
    // 分配内存
    float *A, *B, *C;
    cudaMalloc(&A, M * K * sizeof(float));
    cudaMalloc(&B, K * N * sizeof(float));
    cudaMalloc(&C, M * N * sizeof(float));
    
    // 预热
    for (int i = 0; i < 10; i++) {
        gemm_kernel<TILE_SIZE, BLOCK_SIZE><<<grid, block>>>(A, B, C, M, N, K);
    }
    
    // 计时
    cudaEvent_t start, stop;
    cudaEventCreate(&start);
    cudaEventCreate(&stop);
    
    cudaEventRecord(start);
    for (int i = 0; i < 100; i++) {
        gemm_kernel<TILE_SIZE, BLOCK_SIZE><<<grid, block>>>(A, B, C, M, N, K);
    }
    cudaEventRecord(stop);
    cudaEventSynchronize(stop);
    
    float ms;
    cudaEventElapsedTime(&ms, start, stop);
    
    float gflops = (2.0 * M * N * K * 100) / (ms * 1e6);
    printf("TILE=%d, BLOCK=%d: %.2f GFLOPS\n", TILE_SIZE, BLOCK_SIZE, gflops);
    
    cudaFree(A); cudaFree(B); cudaFree(C);
}
```

#### 5.2 使用Nsight Compute分析

```bash
# 分析占用率
ncu --metrics sm__warps_active.avg.pct_of_peak_sustained_active \
    ./gemm

# 分析共享内存
ncu --metrics smsp__shared_mem_bytes.sum \
    ./gemm

# 分析寄存器使用
ncu --metrics smsp__sass_average_data_bytes_per_sector_mem_global.pct \
    ./gemm

# 完整分析
ncu --set full -o profile ./gemm
```

**关键指标**：
- `sm__warps_active`: Warp活跃度
- `smsp__shared_mem_bytes`: 共享内存使用
- `smsp__sass_average_registers_per_thread`: 每线程寄存器数
- `sm__throughput.avg.pct_of_peak_sustained_elapsed`: 吞吐量占比

#### 5.3 自适应选择

```cuda
// 根据问题规模自动选择配置
template<int M, int N, int K>
struct GemmConfig {
    static constexpr int TILE_SIZE = 
        (M < 512 || N < 512) ? 16 :
        (M < 2048 || N < 2048) ? 32 : 64;
    
    static constexpr int BLOCK_SIZE = 
        (TILE_SIZE == 16) ? 256 :
        (TILE_SIZE == 32) ? 256 : 128;
    
    static constexpr int THREAD_TILE = 
        TILE_SIZE / 16;
};

// 使用
auto config = GemmConfig<4096, 4096, 4096>();
gemm<config::TILE_SIZE, config::BLOCK_SIZE>(...);
```

### 6. 特殊场景

#### 6.1 Tensor Core配置

```cuda
// WMMA需要warp级别配置
#define WMMA_M 16
#define WMMA_N 16
#define WMMA_K 16

// 每个warp计算一个或多个16×16块
// 线程块配置
dim3 blockDim(32, 4);  // 128 threads = 4 warps

// 或
dim3 blockDim(128);    // 4 warps（1D配置）
```

#### 6.2 小矩阵（< 64）

```cuda
// 矩阵太小，无法充分利用GPU
// 策略1: Batched GEMM
// 策略2: 使用很小的tile
#define TILE_SIZE 8
dim3 blockDim(8, 8);
```

#### 6.3 非方阵

```cuda
// M >> N 或 N >> M
// 使用矩形Tile

// 例如：M=4096, N=64
#define TILE_M 128
#define TILE_N 64
dim3 blockDim(16, 16);
```

### 7. 推荐配置表

#### 7.1 按矩阵大小

| 矩阵规模  | Tile M×N×K | Block Size | 备注     |
| --------- | ---------- | ---------- | -------- |
| < 256     | 16×16×16   | 256        | 小矩阵   |
| 256-1024  | 32×32×32   | 256        | 中等     |
| 1024-4096 | 64×64×8    | 128        | 大矩阵   |
| > 4096    | 128×128×8  | 256        | 超大矩阵 |

#### 7.2 按优化程度

| 优化级别      | Tile    | Block        | Thread Tile  |
| ------------- | ------- | ------------ | ------------ |
| 基础Tiling    | 32×32   | 32×32 (1024) | 1×1          |
| + 寄存器分块  | 32×32   | 8×8 (64)     | 4×4          |
| + 双缓冲      | 64×64   | 16×16 (256)  | 4×4          |
| + Tensor Core | 128×128 | 256 (1D)     | 16×16 (WMMA) |

#### 7.3 按GPU架构

| 架构   | 推荐Tile  | 推荐Block | 说明                |
| ------ | --------- | --------- | ------------------- |
| Pascal | 32×32×32  | 128       | 共享内存48KB        |
| Volta  | 32×32×32  | 128       | 引入Tensor Core     |
| Turing | 32×32×32  | 256       | 改进的Tensor Core   |
| Ampere | 64×64×8   | 128       | 更大共享内存(164KB) |
| Hopper | 128×128×8 | 256       | 异步DMA             |

### 8. 调试检查清单

配置验证：
- [ ] Block size是32的倍数
- [ ] Block size ≤ 1024
- [ ] 共享内存 < 48KB (或164KB)
- [ ] 寄存器使用合理（< 128/thread）
- [ ] 占用率 > 50%
- [ ] Tile大小适应矩阵规模
- [ ] 没有bank conflict（添加padding）

## 总结

**线程块大小选择原则**：
1. **Warp倍数**：必须是32的倍数
2. **典型范围**：128-256
3. **占用率目标**：> 50%，理想75-100%
4. **资源平衡**：寄存器 + 共享内存

**分块大小选择原则**：
1. **共享内存限制**：< 48KB（或架构限制）
2. **数据复用**：越大越好，但受资源限制
3. **访存效率**：避免bank conflict
4. **矩阵规模适配**：小矩阵小Tile，大矩阵大Tile

**推荐起点**：
```cuda
#define TILE_SIZE 32
dim3 blockDim(16, 16);  // 256 threads
// 然后根据性能分析调整
```

**调优流程**：
1. 从推荐配置开始
2. 使用Occupancy Calculator验证
3. 系统性benchmark多个配置
4. 用Nsight Compute分析瓶颈
5. 根据分析结果迭代优化

**关键认识**：
- 没有一刀切的最优配置
- 需要根据具体问题调优
- 工具辅助比经验更可靠
- 实测胜于理论分析

选择合适的参数是GEMM优化的**基础**，需要理论分析和实验验证相结合。


---

## 相关笔记
<!-- 自动生成 -->

暂无相关笔记

