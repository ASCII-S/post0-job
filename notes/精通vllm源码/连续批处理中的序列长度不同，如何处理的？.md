---
created: '2025-11-13'
last_reviewed: null
next_review: '2025-11-13'
review_count: 0
difficulty: medium
mastery_level: 0.0
tags:
- 精通vllm源码
- 精通vllm源码/连续批处理中的序列长度不同，如何处理的？.md
related_outlines: []
---
# 连续批处理中的序列长度不同，如何处理的？

## 面试标准答案

vLLM通过 **PagedAttention** 和 **动态内存管理** 来高效处理不同序列长度的批处理。核心思想是：**不使用padding**，而是将每个序列的KV Cache分散存储在不连续的内存块（pages）中，通过逻辑到物理地址的映射表管理。这样可以实现：(1) 零内存浪费（无padding开销）；(2) 动态批处理调度；(3) 高效内存共享。具体实现在 `vllm.core.block_manager` 和 `vllm.attention.backends` 模块中。

## 详细解析

### 1. 传统方法的问题

在传统的批处理推理中，处理不同长度序列需要padding到相同长度：

```python
# 传统方法的问题
batch = [
    "Hello",           # 长度: 5
    "Hello world",     # 长度: 10  
    "Hello world!"     # 长度: 11
]

# 需要padding到最大长度11
padded_batch = [
    "Hello      ",     # 浪费6个token的内存和计算
    "Hello world",     # 浪费1个token
    "Hello world!"     # 无浪费
]
# 总浪费率: 7/33 ≈ 21%
```

这导致：
- **内存浪费**：padding部分占用KV Cache空间
- **计算浪费**：attention需要处理padding token
- **批处理效率低**：必须等待最长序列完成才能加入新请求

### 2. vLLM的解决方案：PagedAttention

#### 2.1 核心思想

vLLM借鉴操作系统的虚拟内存分页机制：

```
传统连续存储:
┌────────────────────────────────────┐
│ Seq1: ████████░░░░ (8 used, 4 pad) │
│ Seq2: ████░░░░░░░░ (4 used, 8 pad) │
└────────────────────────────────────┘
浪费12个位置

PagedAttention分页存储:
┌─────┬─────┬─────┬─────┐
│ P0  │ P1  │ P2  │ P3  │ Physical Blocks
└─────┴─────┴─────┴─────┘
  ↑     ↑     ↑     ↑
  │     │     │     └── Seq2: [P3] (4 tokens)
  │     └─────┴──────── Seq1: [P1, P2] (8 tokens)
  └──────────────────── Free
零浪费！
```

#### 2.2 关键数据结构

**Block Table（块表）**：

```python
# vllm/core/block_manager.py
class BlockTable:
    """逻辑块到物理块的映射表"""
    
    # 示例：一个序列的block table
    sequence_id: int = 123
    logical_blocks: List[int] = [0, 1, 2]      # 逻辑块索引
    physical_blocks: List[int] = [5, 12, 7]    # 物理块索引
    
    # 逻辑块0 -> 物理块5
    # 逻辑块1 -> 物理块12
    # 逻辑块2 -> 物理块7
```

**Block Manager（块管理器）**：

```python
class BlockSpaceManager:
    """管理KV Cache的物理内存分配"""
    
    def __init__(self, block_size: int, num_blocks: int):
        self.block_size = 16  # 每个块存16个token的KV
        self.free_blocks = set(range(num_blocks))  # 空闲块池
        self.block_tables = {}  # seq_id -> BlockTable
    
    def allocate(self, seq_id: int, num_tokens: int):
        """为序列分配物理块"""
        num_blocks_needed = (num_tokens + self.block_size - 1) // self.block_size
        allocated = []
        for _ in range(num_blocks_needed):
            block = self.free_blocks.pop()
            allocated.append(block)
        self.block_tables[seq_id] = allocated
        return allocated
    
    def free(self, seq_id: int):
        """释放序列占用的块"""
        blocks = self.block_tables.pop(seq_id)
        self.free_blocks.update(blocks)
```

### 3. 处理流程详解

#### 3.1 请求到达

```python
# vllm/engine/llm_engine.py
class LLMEngine:
    def add_request(self, request: Request):
        # 1. 创建序列对象
        seq = Sequence(
            seq_id=request.request_id,
            prompt_tokens=request.prompt,
            max_length=request.max_tokens
        )
        
        # 2. 为序列分配初始块（只为prompt分配）
        num_prompt_tokens = len(request.prompt)
        self.block_manager.allocate(seq.seq_id, num_prompt_tokens)
        
        # 3. 加入调度队列
        self.scheduler.add_seq(seq)
```

#### 3.2 调度与批处理

```python
# vllm/core/scheduler.py
class Scheduler:
    def schedule(self) -> SchedulerOutputs:
        """动态调度：选择可以一起执行的序列"""
        running_seqs = []
        
        # 从等待队列选择序列，直到GPU内存满
        while self.waiting_queue and self.has_free_blocks():
            seq = self.waiting_queue.pop(0)
            
            # 检查是否有足够的物理块
            if self.can_allocate(seq):
                self.allocate_blocks(seq)
                running_seqs.append(seq)
            else:
                break  # 内存不足，停止添加
        
        # 关键：不同长度的序列可以在同一批次！
        return SchedulerOutputs(
            scheduled_seqs=running_seqs,  # 长度可以不同
            num_batched_tokens=sum(seq.get_len() for seq in running_seqs)
        )
```

#### 3.3 Attention计算

```python
# vllm/attention/backends/xformers.py
def paged_attention(
    query: Tensor,           # [num_seqs, num_heads, head_size]
    key_cache: Tensor,       # [num_blocks, block_size, num_heads, head_size]
    value_cache: Tensor,     # [num_blocks, block_size, num_heads, head_size]
    block_tables: Tensor,    # [num_seqs, max_num_blocks_per_seq]
    context_lens: Tensor,    # [num_seqs] - 每个序列的实际长度
):
    """
    使用block_tables将逻辑位置映射到物理位置
    """
    num_seqs = query.shape[0]
    outputs = []
    
    for seq_idx in range(num_seqs):
        # 获取该序列的块表和实际长度
        seq_block_table = block_tables[seq_idx]
        seq_len = context_lens[seq_idx]
        
        # 根据块表收集KV
        seq_keys = []
        seq_values = []
        for block_idx in seq_block_table:
            if block_idx >= 0:  # -1表示无效块
                seq_keys.append(key_cache[block_idx])
                seq_values.append(value_cache[block_idx])
        
        # 拼接并截取到实际长度
        seq_keys = torch.cat(seq_keys, dim=0)[:seq_len]
        seq_values = torch.cat(seq_values, dim=0)[:seq_len]
        
        # 标准attention计算
        attn_output = scaled_dot_product_attention(
            query[seq_idx], 
            seq_keys, 
            seq_values
        )
        outputs.append(attn_output)
    
    return torch.stack(outputs)
```

### 4. 动态长度管理

#### 4.1 序列增长

```python
# 每生成一个新token
def append_token(self, seq: Sequence, token_id: int):
    seq.append_token(token_id)
    
    # 检查当前块是否已满
    current_block_idx = len(seq.logical_blocks) - 1
    current_block = seq.logical_blocks[current_block_idx]
    
    if current_block.is_full():
        # 分配新的物理块
        new_block = self.block_manager.allocate_block()
        seq.logical_blocks.append(new_block)
```

#### 4.2 序列完成

```python
def handle_finished_seq(self, seq: Sequence):
    # 立即释放物理块
    self.block_manager.free(seq.seq_id)
    
    # 释放的块可以立即给新请求使用
    # 不需要等待整个批次完成！
```

### 5. 内存效率对比

#### 实际案例分析

假设batch中有3个请求：

| 请求 | Prompt长度 | 生成长度 | 总长度 |
| ---- | ---------- | -------- | ------ |
| A    | 50         | 10       | 60     |
| B    | 120        | 80       | 200    |
| C    | 30         | 5        | 35     |

**传统方法（padding）**：
```
需要padding到max_len=200
总内存 = 200 × 3 = 600个token位置
实际使用 = 60 + 200 + 35 = 295
浪费率 = (600-295)/600 = 50.8%
```

**vLLM方法（PagedAttention）**：
```
Block size = 16
A需要: ⌈60/16⌉ = 4块
B需要: ⌈200/16⌉ = 13块  
C需要: ⌈35/16⌉ = 3块
总内存 = (4+13+3) × 16 = 320个token位置
实际使用 = 295
浪费率 = (320-295)/320 = 7.8%
```

**内存节省**: 50.8% → 7.8%，减少了 **84%** 的内存浪费！

### 6. 代码实现位置

#### 核心模块

```python
vllm/
├── core/
│   ├── block_manager.py           # BlockSpaceManager核心实现
│   ├── scheduler.py               # 动态调度逻辑
│   └── policy.py                  # 调度策略（FCFS等）
├── attention/
│   ├── backends/
│   │   ├── xformers.py           # xFormers后端的PagedAttention
│   │   ├── flash_attn.py         # FlashAttention后端
│   │   └── torch_native.py       # PyTorch原生后端
│   └── ops/
│       └── paged_attn.py         # PagedAttention CUDA kernel
└── worker/
    └── model_runner.py            # 执行推理的worker
```

#### 关键代码片段

查看BlockSpaceManager的完整实现：
```bash
# 核心的块管理逻辑
vllm/core/block_manager.py:BlockSpaceManager

# PagedAttention的CUDA实现  
vllm/attention/ops/paged_attn.py:paged_attention_v1
vllm/attention/ops/paged_attn.py:paged_attention_v2

# 调度器如何处理变长序列
vllm/core/scheduler.py:Scheduler.schedule()
```

### 7. 性能优化技巧

#### 7.1 Block Size选择

```python
# Block size的权衡
block_size = 16  # vLLM默认值

# 太小(如8): 管理开销大，块表查找多
# 太大(如64): 内存浪费增加（最后一块可能只用一部分）
# 16是经验最优值
```

#### 7.2 预分配策略

```python
# 预分配prompt的块，但生成阶段按需分配
def allocate_prompt_blocks(self, seq: Sequence):
    """一次性分配整个prompt需要的块"""
    num_blocks = (len(seq.prompt) + self.block_size - 1) // self.block_size
    return self.allocate_blocks(num_blocks)

def allocate_generation_blocks(self, seq: Sequence):
    """生成阶段逐块分配，避免为max_length预留"""
    if seq.last_block_is_full():
        return self.allocate_blocks(1)
```

#### 7.3 内存共享

```python
# 对于beam search或并行采样，共享prompt的KV Cache
def fork_sequence(self, parent_seq: Sequence) -> Sequence:
    """创建子序列，共享父序列的块"""
    child_seq = Sequence(...)
    
    # 共享父序列的所有块（copy-on-write）
    child_seq.block_table = parent_seq.block_table.copy()
    
    # 增加引用计数
    for block in child_seq.block_table:
        self.block_refcount[block] += 1
```

### 8. 与其他框架对比

| 框架                  | 变长序列处理              | 内存效率         | 动态批处理 |
| --------------------- | ------------------------- | ---------------- | ---------- |
| **vLLM**              | PagedAttention            | 极高（~7%浪费）  | ✅ 完全支持 |
| **TGI**               | Flash Attention + Padding | 中等（~30%浪费） | ⚠️ 部分支持 |
| **FasterTransformer** | Static Batching + Padding | 低（~50%浪费）   | ❌ 不支持   |
| **TensorRT-LLM**      | In-flight Batching        | 高（~15%浪费）   | ✅ 支持     |

### 9. 调试与监控

```python
# 查看当前内存使用情况
def get_memory_stats(self):
    total_blocks = len(self.physical_blocks)
    used_blocks = total_blocks - len(self.free_blocks)
    
    print(f"Block utilization: {used_blocks}/{total_blocks} "
          f"({used_blocks/total_blocks*100:.1f}%)")
    
    # 查看每个序列的块使用
    for seq_id, block_table in self.block_tables.items():
        seq_len = self.sequences[seq_id].get_len()
        num_blocks = len(block_table)
        utilization = seq_len / (num_blocks * self.block_size)
        print(f"Seq {seq_id}: {num_blocks} blocks, "
              f"utilization={utilization*100:.1f}%")
```

### 10. 常见问题与解决

#### Q1: 为什么不直接使用变长tensor？

**答**: GPU kernel需要规则的内存访问模式。PagedAttention通过固定大小的block + 间接寻址，既保持了kernel效率，又实现了逻辑上的变长。

#### Q2: Block碎片化怎么办？

**答**: vLLM会尽量复用释放的块。如果碎片严重，可以通过调整block_size或启用内存压缩（swapping）。

#### Q3: 如何处理超长序列？

**答**: 超过GPU内存时，vLLM支持将部分块swap到CPU内存：
```python
# vllm/core/block_manager.py
def swap_out(self, seq_id: int):
    """将序列的块从GPU swap到CPU"""
    pass
```

## 总结

vLLM通过PagedAttention实现了对变长序列的高效处理，核心优势：

1. **零Padding浪费**：每个序列只占用实际需要的内存
2. **动态批处理**：序列完成立即释放，新请求立即加入
3. **内存共享**：beam search等场景可共享KV Cache
4. **高吞吐量**：更高的内存利用率 = 更大的批处理 = 更高吞吐

这是vLLM相比传统推理框架的核心竞争力之一，特别适合在线服务场景。

## 参考文献

1. [Efficient Memory Management for Large Language Model Serving with PagedAttention (vLLM Paper)](https://arxiv.org/abs/2309.06180)
2. [vLLM Official Documentation - Architecture Overview](https://docs.vllm.ai/en/latest/dev/arch_overview.html)
3. [vLLM GitHub - Block Manager Implementation](https://github.com/vllm-project/vllm/blob/main/vllm/core/block_manager.py)
4. [vLLM GitHub - PagedAttention Kernels](https://github.com/vllm-project/vllm/tree/main/csrc/attention)
5. [Blog: Understanding vLLM's PagedAttention](https://blog.vllm.ai/2023/06/20/vllm.html)
6. [FlashAttention: Fast and Memory-Efficient Exact Attention](https://arxiv.org/abs/2205.14135)
7. [Orca: A Distributed Serving System for Transformer-Based Generative Models](https://www.usenix.org/conference/osdi22/presentation/yu)

